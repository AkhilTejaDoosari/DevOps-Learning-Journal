# **09. Text Processing — Searching, Filtering, and Manipulating Text**
> Mastering grep, sed, awk, and text manipulation tools for log analysis and data processing.

---

## Table of Contents
- [1. Why Text Processing Matters](#1-why-text-processing-matters)
- [2. Understanding Unix Text Philosophy](#2-understanding-unix-text-philosophy)
- [3. Searching with grep](#3-searching-with-grep)
- [4. Advanced grep Patterns](#4-advanced-grep-patterns)
- [5. Context and Line Numbers](#5-context-and-line-numbers)
- [6. Stream Editing with sed](#6-stream-editing-with-sed)
- [7. Advanced sed Operations](#7-advanced-sed-operations)
- [8. Pattern Processing with awk](#8-pattern-processing-with-awk)
- [9. awk Variables and Built-ins](#9-awk-variables-and-built-ins)
- [10. Sorting and Uniqueness](#10-sorting-and-uniqueness)
- [11. Cutting and Pasting](#11-cutting-and-pasting)
- [12. Translating Characters](#12-translating-characters)
- [13. Word and Line Counting](#13-word-and-line-counting)
- [14. Comparing Files](#14-comparing-files)
- [15. Combining Tools with Pipes](#15-combining-tools-with-pipes)
- [16. Real-World Scenarios](#16-real-world-scenarios)
- [17. Commands Reference](#17-commands-reference)
- [18. Quick Reference](#18-quick-reference)

---

<details>
<summary><strong>1. Why Text Processing Matters</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Every DevOps task involves text processing.

Whether it's:
- Analyzing logs for errors
- Extracting data from API responses
- Processing CSV files
- Filtering configuration files
- Monitoring system metrics

Understanding text processing means:
- you can find critical errors in logs instantly
- you can transform data formats on the fly
- you can automate report generation
- you can process large files efficiently

This isn't about memorizing regex syntax.
It's about understanding how Unix tools process text streams and how to chain them together.

</div>

</details>

---

<details>
<summary><strong>2. Understanding Unix Text Philosophy</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-001 — Text as Universal Interface

Unix philosophy: everything is text.

**Why Text?**
- Human-readable
- Easy to process
- Chain tools together
- Universal format

**The Pipeline Model:**
```
Input → Filter → Filter → Filter → Output
```

**Example:**
```bash copy
cat access.log | grep "ERROR" | awk '{print $1}' | sort | uniq -c
```

Each tool does one thing well, outputs text for the next tool.

**Key Insight:**
Most Unix tools work with **streams of text lines**.
Learn to think in terms of line-by-line processing.

</div>

</details>

---

<details>
<summary><strong>3. Searching with grep</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-002 — grep Basics

**grep** searches for patterns in text.

**Basic syntax:**
```bash copy
grep "pattern" filename
```

**Search in multiple files:**
```bash copy
grep "error" *.log
```

**Case-insensitive search:**
```bash copy
grep -i "error" file.txt
```

**Invert match (show non-matching lines):**
```bash copy
grep -v "DEBUG" file.txt
```

**Show only matching text:**
```bash copy
grep -o "error" file.txt
```

**Count matches:**
```bash copy
grep -c "error" file.txt
```

**Recursive search in directories:**
```bash copy
grep -r "error" /var/log/
```

---

### TXT-003 — grep with Pipes

**Most common use: filtering command output**

```bash copy
ps aux | grep nginx
```

```bash copy
dmesg | grep -i error
```

```bash copy
journalctl | grep "failed"
```

**Why grep is powerful:**
Works with any text stream, not just files.

</div>

</details>

---

<details>
<summary><strong>4. Advanced grep Patterns</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-004 — Regular Expressions

**grep** supports regular expressions for complex patterns.

**Basic Regex:**
- `.` — any single character
- `*` — zero or more of previous
- `^` — start of line
- `$` — end of line
- `[]` — character class
- `\` — escape special chars

**Examples:**

**Lines starting with "Error":**
```bash copy
grep "^Error" file.txt
```

**Lines ending with "failed":**
```bash copy
grep "failed$" file.txt
```

**IP addresses (simple pattern):**
```bash copy
grep -E "[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+" file.txt
```

**Email addresses:**
```bash copy
grep -E "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}" file.txt
```

---

### TXT-005 — Extended Regex (grep -E)

**Extended regex adds:**
- `+` — one or more
- `?` — zero or one
- `|` — alternation (OR)
- `()` — grouping

```bash copy
grep -E "error|warning|critical" file.txt
# Matches lines with error OR warning OR critical
```

```bash copy
grep -E "failed [0-9]+ times" file.txt
# Matches "failed 3 times", "failed 42 times", etc.
```

**Alternative:** Use `egrep` (same as `grep -E`)

</div>

</details>

---

<details>
<summary><strong>5. Context and Line Numbers</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-006 — Context Lines

**Show lines before match:**
```bash copy
grep -B 3 "error" file.txt
# Shows 3 lines Before matching line
```

**Show lines after match:**
```bash copy
grep -A 3 "error" file.txt
# Shows 3 lines After matching line
```

**Show lines before and after:**
```bash copy
grep -C 3 "error" file.txt
# Shows 3 lines of Context (before and after)
```

**Why this matters:**
Logs often need context to understand errors.

---

### TXT-007 — Line Numbers

**Show line numbers:**
```bash copy
grep -n "error" file.txt
# Output: 42:This line has an error
```

**Show filename with matches:**
```bash copy
grep -H "error" *.log
# Output: app.log:This line has an error
```

**Suppress filename:**
```bash copy
grep -h "error" *.log
```

</div>

</details>

---

<details>
<summary><strong>6. Stream Editing with sed</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-008 — sed Basics

**sed** (stream editor) transforms text.

**Basic syntax:**
```bash copy
sed 's/old/new/' file.txt
```

**`s/old/new/`** means "substitute old with new"

**Replace first occurrence per line:**
```bash copy
sed 's/error/ERROR/' file.txt
```

**Replace all occurrences per line:**
```bash copy
sed 's/error/ERROR/g' file.txt
```

**Case-insensitive replace:**
```bash copy
sed 's/error/ERROR/gi' file.txt
```

**In-place editing (modify file):**
```bash copy
sed -i 's/error/ERROR/g' file.txt
```

**Backup before editing:**
```bash copy
sed -i.bak 's/error/ERROR/g' file.txt
# Creates file.txt.bak
```

---

### TXT-009 — sed Delete and Print

**Delete lines matching pattern:**
```bash copy
sed '/DEBUG/d' file.txt
```

**Delete lines NOT matching pattern:**
```bash copy
sed '/ERROR/!d' file.txt
# Only shows lines with ERROR
```

**Print specific line:**
```bash copy
sed -n '5p' file.txt
# Prints line 5
```

**Print line range:**
```bash copy
sed -n '10,20p' file.txt
# Prints lines 10 through 20
```

**Print lines matching pattern:**
```bash copy
sed -n '/error/p' file.txt
# Same as grep "error" file.txt
```

</div>

</details>

---

<details>
<summary><strong>7. Advanced sed Operations</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-010 — Multiple Operations

**Chain multiple sed commands:**
```bash copy
sed -e 's/error/ERROR/g' -e 's/warning/WARNING/g' file.txt
```

**Using semicolon:**
```bash copy
sed 's/error/ERROR/g; s/warning/WARNING/g' file.txt
```

---

### TXT-011 — Address Ranges

**Replace only on specific lines:**
```bash copy
sed '10,20s/old/new/g' file.txt
# Replace on lines 10-20 only
```

**Replace from line to end:**
```bash copy
sed '50,$s/old/new/g' file.txt
# From line 50 to end of file
```

---

### TXT-012 — Advanced Substitution

**Use different delimiter:**
```bash copy
sed 's|/old/path|/new/path|g' file.txt
# Useful for paths (avoids escaping /)
```

**Backreferences:**
```bash copy
sed 's/\([0-9]*\)/Number: \1/g' file.txt
# Wraps numbers in "Number: "
```

**Delete blank lines:**
```bash copy
sed '/^$/d' file.txt
```

**Delete trailing whitespace:**
```bash copy
sed 's/[[:space:]]*$//' file.txt
```

</div>

</details>

---

<details>
<summary><strong>8. Pattern Processing with awk</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-013 — awk Basics

**awk** processes text by fields (columns).

**Basic syntax:**
```bash copy
awk '{print $1}' file.txt
# Prints first field (column) of each line
```

**Default field separator: whitespace**

**Print multiple fields:**
```bash copy
awk '{print $1, $3}' file.txt
```

**Print entire line:**
```bash copy
awk '{print $0}' file.txt
```

**Print with custom separator:**
```bash copy
awk '{print $1 " - " $3}' file.txt
```

---

### TXT-014 — Custom Field Separator

**Using comma as separator:**
```bash copy
awk -F',' '{print $1, $2}' file.csv
```

**Using colon (like /etc/passwd):**
```bash copy
awk -F':' '{print $1, $6}' /etc/passwd
# Prints username and home directory
```

**Multiple separators:**
```bash copy
awk -F'[,:]' '{print $1}' file.txt
```

---

### TXT-015 — Pattern Matching

**Print lines matching pattern:**
```bash copy
awk '/error/ {print}' file.txt
# Same as grep "error" file.txt
```

**Print specific field when matching:**
```bash copy
awk '/error/ {print $1, $4}' file.txt
```

**Pattern on specific field:**
```bash copy
awk '$3 == "ERROR" {print $0}' file.txt
# Only if third field equals "ERROR"
```

**Numeric comparison:**
```bash copy
awk '$4 > 100 {print $1, $4}' file.txt
# Print when 4th field > 100
```

</div>

</details>

---

<details>
<summary><strong>9. awk Variables and Built-ins</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-016 — Built-in Variables

**Common awk variables:**
- `NR` — current line number
- `NF` — number of fields in current line
- `FS` — field separator (input)
- `OFS` — output field separator
- `$0` — entire line
- `$1, $2, $3...` — individual fields

**Print line numbers:**
```bash copy
awk '{print NR, $0}' file.txt
```

**Print number of fields:**
```bash copy
awk '{print NF}' file.txt
```

**Print last field:**
```bash copy
awk '{print $NF}' file.txt
```

**Print second-to-last field:**
```bash copy
awk '{print $(NF-1)}' file.txt
```

---

### TXT-017 — BEGIN and END

**BEGIN block:** Runs before processing lines
**END block:** Runs after processing lines

**Example: Sum numbers:**
```bash copy
awk '{sum += $1} END {print sum}' file.txt
```

**Count lines:**
```bash copy
awk 'END {print NR}' file.txt
# Same as wc -l
```

**Header and footer:**
```bash copy
awk 'BEGIN {print "Report Start"} {print $0} END {print "Report End"}' file.txt
```

---

### TXT-018 — Calculations

**Average:**
```bash copy
awk '{sum += $1; count++} END {print sum/count}' file.txt
```

**Find max:**
```bash copy
awk 'BEGIN {max=0} {if ($1 > max) max=$1} END {print max}' file.txt
```

**Count occurrences:**
```bash copy
awk '{count[$1]++} END {for (word in count) print word, count[word]}' file.txt
```

</div>

</details>

---

<details>
<summary><strong>10. Sorting and Uniqueness</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-019 — sort Command

**Basic sort:**
```bash copy
sort file.txt
```

**Reverse sort:**
```bash copy
sort -r file.txt
```

**Numeric sort:**
```bash copy
sort -n file.txt
```

**Sort by specific column:**
```bash copy
sort -k2 file.txt
# Sort by 2nd column
```

**Numeric sort by column:**
```bash copy
sort -k3 -n file.txt
# Sort by 3rd column numerically
```

**Sort by multiple columns:**
```bash copy
sort -k1,1 -k2,2n file.txt
# Sort by 1st column (text), then 2nd (numeric)
```

**Unique sort:**
```bash copy
sort -u file.txt
# Same as sort file.txt | uniq
```

---

### TXT-020 — uniq Command

**Remove duplicate consecutive lines:**
```bash copy
uniq file.txt
```

**Important:** `uniq` only removes consecutive duplicates.
Always sort first!

**Remove duplicates (proper way):**
```bash copy
sort file.txt | uniq
```

**Count occurrences:**
```bash copy
sort file.txt | uniq -c
```

**Show only duplicates:**
```bash copy
sort file.txt | uniq -d
```

**Show only unique lines:**
```bash copy
sort file.txt | uniq -u
```

</div>

</details>

---

<details>
<summary><strong>11. Cutting and Pasting</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-021 — cut Command

**Extract specific columns.**

**By character position:**
```bash copy
cut -c1-5 file.txt
# Characters 1 through 5
```

**By field (delimiter-based):**
```bash copy
cut -d',' -f1,3 file.csv
# Fields 1 and 3, comma-delimited
```

**By field range:**
```bash copy
cut -d':' -f1-3 /etc/passwd
# Fields 1 through 3
```

**All fields from 3 onward:**
```bash copy
cut -d' ' -f3- file.txt
```

---

### TXT-022 — paste Command

**Merge lines from multiple files side-by-side.**

```bash copy
paste file1.txt file2.txt
```

**Custom delimiter:**
```bash copy
paste -d',' file1.txt file2.txt
# Output: line1_file1,line1_file2
```

**Serial paste (one file after another):**
```bash copy
paste -s file1.txt file2.txt
```

</div>

</details>

---

<details>
<summary><strong>12. Translating Characters</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-023 — tr Command

**Translate or delete characters.**

**Lowercase to uppercase:**
```bash copy
tr 'a-z' 'A-Z' < file.txt
```

**Uppercase to lowercase:**
```bash copy
tr 'A-Z' 'a-z' < file.txt
```

**Delete characters:**
```bash copy
tr -d '0-9' < file.txt
# Removes all digits
```

**Squeeze repeats:**
```bash copy
tr -s ' ' < file.txt
# Squeezes multiple spaces to single space
```

**Replace spaces with newlines:**
```bash copy
tr ' ' '\n' < file.txt
```

**Remove newlines:**
```bash copy
tr -d '\n' < file.txt
```

</div>

</details>

---

<details>
<summary><strong>13. Word and Line Counting</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-024 — wc Command

**Count lines, words, characters.**

**Count lines:**
```bash copy
wc -l file.txt
```

**Count words:**
```bash copy
wc -w file.txt
```

**Count characters:**
```bash copy
wc -c file.txt
```

**Count bytes:**
```bash copy
wc -c file.txt
```

**All counts:**
```bash copy
wc file.txt
# Output: lines words characters filename
```

**Common use: count results:**
```bash copy
grep "error" file.txt | wc -l
# How many lines contain "error"
```

</div>

</details>

---

<details>
<summary><strong>14. Comparing Files</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-025 — diff Command

**Show differences between files.**

```bash copy
diff file1.txt file2.txt
```

**Side-by-side comparison:**
```bash copy
diff -y file1.txt file2.txt
```

**Unified format (like git diff):**
```bash copy
diff -u file1.txt file2.txt
```

**Ignore whitespace:**
```bash copy
diff -w file1.txt file2.txt
```

**Brief mode (just say if different):**
```bash copy
diff -q file1.txt file2.txt
```

---

### TXT-026 — comm Command

**Compare sorted files line by line.**

```bash copy
comm file1.txt file2.txt
```

**Output:**
- Column 1: lines only in file1
- Column 2: lines only in file2
- Column 3: lines in both files

**Show only unique to file1:**
```bash copy
comm -23 file1.txt file2.txt
```

**Show only unique to file2:**
```bash copy
comm -13 file1.txt file2.txt
```

**Show only common lines:**
```bash copy
comm -12 file1.txt file2.txt
```

**Important:** Files must be sorted!

</div>

</details>

---

<details>
<summary><strong>15. Combining Tools with Pipes</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### TXT-027 — Pipeline Examples

**Most common access log IPs:**
```bash copy
awk '{print $1}' access.log | sort | uniq -c | sort -rn | head -10
```

**Find largest directories:**
```bash copy
du -h / | sort -rh | head -20
```

**Count error types:**
```bash copy
grep "ERROR" app.log | awk '{print $5}' | sort | uniq -c | sort -rn
```

**Extract email addresses:**
```bash copy
grep -Eo "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}" file.txt | sort -u
```

**Top CPU consumers:**
```bash copy
ps aux | awk '{print $3, $11}' | sort -rn | head -10
```

**Word frequency:**
```bash copy
cat file.txt | tr -s ' ' '\n' | tr 'A-Z' 'a-z' | sort | uniq -c | sort -rn
```

---

### TXT-028 — Complex Processing

**Apache log analysis:**
```bash copy
# Top 10 URLs
awk '{print $7}' access.log | sort | uniq -c | sort -rn | head -10

# 404 errors
awk '$9 == 404 {print $7}' access.log | sort | uniq -c

# Traffic by hour
awk '{print $4}' access.log | cut -d: -f2 | sort | uniq -c
```

**CSV processing:**
```bash copy
# Extract column 2 and 5 from CSV
cut -d',' -f2,5 data.csv

# Sort CSV by column 3 (numeric)
sort -t',' -k3 -n data.csv

# Count unique values in column 1
cut -d',' -f1 data.csv | sort | uniq -c
```

</div>

</details>

---

<details>
<summary><strong>16. Real-World Scenarios</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

### Scenario 1: Find All Error Messages in Logs

**Symptom:**
Application failing, need to find error patterns.

**Cause:**
Errors buried in large log file.

**Fix:**
```bash copy
# Simple search
grep -i "error" /var/log/app.log

# With context
grep -C 3 -i "error" /var/log/app.log

# Count error types
grep -i "error" /var/log/app.log | awk '{print $5}' | sort | uniq -c | sort -rn

# Errors in last hour (with timestamps)
grep "$(date -d '1 hour ago' '+%Y-%m-%d %H')" /var/log/app.log | grep -i error
```

**Confirm:**
Found error patterns, can now investigate root cause.

---

### Scenario 2: Extract IP Addresses from Access Log

**Symptom:**
Need to identify top traffic sources.

**Cause:**
Analyzing web traffic patterns.

**Fix:**
```bash copy
# Top 10 IPs by request count
awk '{print $1}' access.log | sort | uniq -c | sort -rn | head -10

# IPs with more than 1000 requests
awk '{print $1}' access.log | sort | uniq -c | awk '$1 > 1000 {print $2, $1}'

# Unique IPs count
awk '{print $1}' access.log | sort -u | wc -l
```

**Confirm:**
Identified heavy traffic sources.

---

### Scenario 3: Replace Configuration Value

**Symptom:**
Need to update server port in config file.

**Cause:**
Configuration change required.

**Fix:**
```bash copy
# Preview change
sed -n 's/port=8080/port=9000/p' config.ini

# Apply change (with backup)
sed -i.bak 's/port=8080/port=9000/g' config.ini

# Verify
grep "port=" config.ini
```

**Confirm:**
```bash copy
cat config.ini | grep port
# Output: port=9000
```

---

### Scenario 4: Process CSV Data

**Symptom:**
Need to extract and analyze CSV columns.

**Cause:**
Data analysis task.

**Fix:**
```bash copy
# Extract columns 2 and 4
cut -d',' -f2,4 data.csv

# Sort by column 3 (numeric)
sort -t',' -k3 -n data.csv

# Sum column 5
awk -F',' '{sum += $5} END {print sum}' data.csv

# Count unique values in column 1
cut -d',' -f1 data.csv | sort | uniq | wc -l
```

**Confirm:**
Extracted and processed data successfully.

---

### Scenario 5: Find Duplicate Lines

**Symptom:**
Need to identify duplicate entries in file.

**Cause:**
Data validation task.

**Fix:**
```bash copy
# Show duplicate lines
sort file.txt | uniq -d

# Count duplicates
sort file.txt | uniq -c | sort -rn

# Remove duplicates (keep one)
sort file.txt | uniq > file_unique.txt
```

**Confirm:**
```bash copy
wc -l file.txt file_unique.txt
# Shows original vs deduplicated count
```

---

### Scenario 6: Extract Data Between Patterns

**Symptom:**
Need to extract text between START and END markers.

**Cause:**
Log file has sections marked by tags.

**Fix:**
```bash copy
# Extract lines between markers
sed -n '/START/,/END/p' file.txt

# Extract and exclude markers
sed -n '/START/,/END/{/START/d;/END/d;p}' file.txt

# Using awk
awk '/START/,/END/' file.txt
```

**Confirm:**
Extracted section successfully.

---

### Scenario 7: Count HTTP Status Codes

**Symptom:**
Need to analyze web server response codes.

**Cause:**
Performance analysis.

**Fix:**
```bash copy
# Count status codes
awk '{print $9}' access.log | sort | uniq -c | sort -rn

# Only 5xx errors
awk '$9 ~ /^5/ {print $9}' access.log | sort | uniq -c

# Status code distribution
awk '{codes[$9]++} END {for (code in codes) print code, codes[code]}' access.log | sort
```

**Confirm:**
Identified error rate and response distribution.

</div>

</details>

---

<details>
<summary><strong>17. Commands Reference</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

**grep:**

```bash copy
grep "pattern" file.txt   # search for pattern
```

```bash copy
grep -i "pattern" file.txt   # case-insensitive
```

```bash copy
grep -v "pattern" file.txt   # invert match
```

```bash copy
grep -r "pattern" /path   # recursive search
```

```bash copy
grep -n "pattern" file.txt   # show line numbers
```

```bash copy
grep -c "pattern" file.txt   # count matches
```

```bash copy
grep -l "pattern" *.txt   # list files with matches
```

```bash copy
grep -o "pattern" file.txt   # show only matching part
```

```bash copy
grep -A 3 "pattern" file.txt   # show 3 lines after
```

```bash copy
grep -B 3 "pattern" file.txt   # show 3 lines before
```

```bash copy
grep -C 3 "pattern" file.txt   # show 3 lines context
```

```bash copy
grep -E "pat1|pat2" file.txt   # extended regex (OR)
```

**sed:**

```bash copy
sed 's/old/new/' file.txt   # replace first occurrence
```

```bash copy
sed 's/old/new/g' file.txt   # replace all occurrences
```

```bash copy
sed 's/old/new/gi' file.txt   # case-insensitive replace
```

```bash copy
sed -i 's/old/new/g' file.txt   # in-place edit
```

```bash copy
sed -i.bak 's/old/new/g' file.txt   # in-place with backup
```

```bash copy
sed '/pattern/d' file.txt   # delete lines matching pattern
```

```bash copy
sed -n '5p' file.txt   # print line 5
```

```bash copy
sed -n '10,20p' file.txt   # print lines 10-20
```

```bash copy
sed -n '/pattern/p' file.txt   # print matching lines
```

```bash copy
sed '/^$/d' file.txt   # delete blank lines
```

**awk:**

```bash copy
awk '{print $1}' file.txt   # print first field
```

```bash copy
awk '{print $1, $3}' file.txt   # print fields 1 and 3
```

```bash copy
awk '{print $NF}' file.txt   # print last field
```

```bash copy
awk -F',' '{print $1}' file.csv   # custom field separator
```

```bash copy
awk '/pattern/ {print}' file.txt   # print matching lines
```

```bash copy
awk '$3 > 100 {print $1}' file.txt   # conditional print
```

```bash copy
awk '{sum += $1} END {print sum}' file.txt   # sum column
```

```bash copy
awk '{print NR, $0}' file.txt   # print with line numbers
```

```bash copy
awk 'BEGIN {FS=":"} {print $1}' file.txt   # set separator in BEGIN
```

**sort:**

```bash copy
sort file.txt   # sort alphabetically
```

```bash copy
sort -r file.txt   # reverse sort
```

```bash copy
sort -n file.txt   # numeric sort
```

```bash copy
sort -k2 file.txt   # sort by column 2
```

```bash copy
sort -k3 -n file.txt   # numeric sort by column 3
```

```bash copy
sort -u file.txt   # sort and remove duplicates
```

```bash copy
sort -t',' -k2 file.csv   # sort CSV by column 2
```

**uniq:**

```bash copy
uniq file.txt   # remove consecutive duplicates
```

```bash copy
sort file.txt | uniq   # remove all duplicates
```

```bash copy
sort file.txt | uniq -c   # count occurrences
```

```bash copy
sort file.txt | uniq -d   # show only duplicates
```

```bash copy
sort file.txt | uniq -u   # show only unique
```

**cut:**

```bash copy
cut -c1-5 file.txt   # extract characters 1-5
```

```bash copy
cut -d',' -f1,3 file.csv   # extract fields 1 and 3
```

```bash copy
cut -d':' -f1 /etc/passwd   # extract first field
```

```bash copy
cut -d' ' -f3- file.txt   # extract from field 3 onward
```

**tr:**

```bash copy
tr 'a-z' 'A-Z' < file.txt   # lowercase to uppercase
```

```bash copy
tr -d '0-9' < file.txt   # delete digits
```

```bash copy
tr -s ' ' < file.txt   # squeeze spaces
```

```bash copy
tr ' ' '\n' < file.txt   # spaces to newlines
```

**wc:**

```bash copy
wc -l file.txt   # count lines
```

```bash copy
wc -w file.txt   # count words
```

```bash copy
wc -c file.txt   # count characters
```

**paste:**

```bash copy
paste file1.txt file2.txt   # merge side-by-side
```

```bash copy
paste -d',' file1.txt file2.txt   # with comma delimiter
```

**diff:**

```bash copy
diff file1.txt file2.txt   # show differences
```

```bash copy
diff -y file1.txt file2.txt   # side-by-side
```

```bash copy
diff -u file1.txt file2.txt   # unified format
```

**comm:**

```bash copy
comm file1.txt file2.txt   # compare sorted files
```

```bash copy
comm -12 file1.txt file2.txt   # show only common lines
```

</div>

</details>

---

<details>
<summary><strong>18. Quick Reference</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

**Key Concepts:**
- Unix treats everything as text streams processed line-by-line
- Pipe tools together: each does one thing well
- `grep` searches, `sed` transforms, `awk` processes columns
- Always `sort` before `uniq` to remove all duplicates
- Regular expressions enable powerful pattern matching
- Most tools read from stdin and write to stdout
- Combine tools with pipes for complex data processing

**Core Text Processing Tools:**

| Tool | Purpose | Primary Use |
|------|---------|-------------|
| `grep` | Search text | Find patterns in files or streams |
| `sed` | Stream editor | Find and replace, delete lines |
| `awk` | Pattern processor | Column extraction, calculations |
| `sort` | Sort lines | Alphabetic or numeric ordering |
| `uniq` | Remove duplicates | Must use after sort |
| `cut` | Extract columns | Split by delimiter or position |
| `tr` | Translate chars | Case conversion, character deletion |
| `wc` | Count | Lines, words, characters |
| `paste` | Merge files | Side-by-side combination |
| `diff` | Compare files | Show differences |

**grep Common Options:**

| Option | Effect | Example |
|--------|--------|---------|
| `-i` | Case-insensitive | `grep -i error file.txt` |
| `-v` | Invert match | `grep -v DEBUG file.txt` |
| `-r` | Recursive | `grep -r error /var/log` |
| `-n` | Line numbers | `grep -n error file.txt` |
| `-c` | Count matches | `grep -c error file.txt` |
| `-l` | List files with matches | `grep -l error *.log` |
| `-A N` | N lines after | `grep -A 3 error file.txt` |
| `-B N` | N lines before | `grep -B 3 error file.txt` |
| `-C N` | N lines context | `grep -C 3 error file.txt` |
| `-E` | Extended regex | `grep -E "err\|warn" file.txt` |

**Basic Regular Expressions:**

| Pattern | Meaning | Example |
|---------|---------|---------|
| `.` | Any single character | `a.c` matches "abc", "a5c" |
| `*` | Zero or more of previous | `ab*` matches "a", "ab", "abb" |
| `^` | Start of line | `^Error` matches "Error at start" |
| `$` | End of line | `done$` matches "Task done" |
| `[]` | Character class | `[abc]` matches "a", "b", or "c" |
| `[^]` | Negated class | `[^0-9]` matches non-digits |
| `\` | Escape special char | `\.` matches literal dot |

**Extended Regular Expressions (grep -E):**

| Pattern | Meaning | Example |
|---------|---------|---------|
| `+` | One or more | `a+` matches "a", "aa", "aaa" |
| `?` | Zero or one | `colou?r` matches "color" or "colour" |
| `\|` | Alternation (OR) | `cat\|dog` matches "cat" or "dog" |
| `()` | Grouping | `(ab)+` matches "ab", "abab" |

**sed Common Operations:**

| Operation | Syntax | Purpose |
|-----------|--------|---------|
| Substitute | `s/old/new/` | Replace first occurrence |
| Global substitute | `s/old/new/g` | Replace all occurrences |
| Delete | `/pattern/d` | Delete matching lines |
| Print | `-n '/pattern/p'` | Print matching lines only |
| In-place edit | `-i` | Modify file directly |
| Line range | `'10,20s/old/new/'` | Apply to lines 10-20 |

**awk Built-in Variables:**

| Variable | Meaning |
|----------|---------|
| `$0` | Entire line |
| `$1, $2, $3...` | Field 1, 2, 3... |
| `$NF` | Last field |
| `NR` | Current line number |
| `NF` | Number of fields in current line |
| `FS` | Input field separator |
| `OFS` | Output field separator |

**awk Common Patterns:**

| Task | Command |
|------|---------|
| Print column 1 | `awk '{print $1}' file.txt` |
| Print last column | `awk '{print $NF}' file.txt` |
| CSV processing | `awk -F',' '{print $1, $3}' file.csv` |
| Conditional print | `awk '$3 > 100 {print}' file.txt` |
| Sum column | `awk '{sum += $1} END {print sum}' file.txt` |
| Count lines | `awk 'END {print NR}' file.txt` |
| Print with line numbers | `awk '{print NR, $0}' file.txt` |

**Common Pipeline Patterns:**

| Task | Pipeline |
|------|----------|
| Top 10 IPs from access log | `awk '{print $1}' access.log \| sort \| uniq -c \| sort -rn \| head -10` |
| Count error types | `grep ERROR app.log \| awk '{print $5}' \| sort \| uniq -c \| sort -rn` |
| Unique email addresses | `grep -Eo "[^@]+@[^@]+" file.txt \| sort -u` |
| Word frequency | `tr -s ' ' '\n' < file.txt \| tr 'A-Z' 'a-z' \| sort \| uniq -c \| sort -rn` |
| Lines containing A but not B | `grep A file.txt \| grep -v B` |
| Remove duplicates | `sort file.txt \| uniq > output.txt` |

**sort Options:**

| Option | Effect |
|--------|--------|
| `-r` | Reverse sort |
| `-n` | Numeric sort |
| `-k N` | Sort by column N |
| `-t DELIM` | Use DELIM as field separator |
| `-u` | Unique (remove duplicates) |
| `-h` | Human-readable numbers (1K, 2M) |

**Text Processing Best Practices:**

1. **Always sort before uniq:** `uniq` only removes consecutive duplicates
2. **Use -E for complex grep:** Extended regex is more powerful
3. **Backup with sed -i.bak:** Never lose original data
4. **Test on small data first:** Verify logic before processing large files
5. **Use awk for columns:** More powerful than cut for field processing
6. **Chain tools with pipes:** Build complex operations from simple parts

**Real-World Use Cases:**

| Scenario | Solution |
|----------|----------|
| Find errors in logs | `grep -i error /var/log/app.log` |
| Top 10 URLs | `awk '{print $7}' access.log \| sort \| uniq -c \| sort -rn \| head -10` |
| Replace config values | `sed -i.bak 's/port=8080/port=9000/g' config.ini` |
| Extract CSV columns | `cut -d',' -f2,5 data.csv` |
| Count HTTP status codes | `awk '{print $9}' access.log \| sort \| uniq -c` |
| Sum numbers in column | `awk '{sum += $3} END {print sum}' data.txt` |

**What's Next:**
Now that you understand text processing, the next file covers networking basics — understanding IPs, ports, DNS, and troubleshooting connectivity issues.

</div>

</details>

---