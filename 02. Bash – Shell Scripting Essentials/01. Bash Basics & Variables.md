# Bash Foundations & Scripting Mindset

It's 3 AM. Your phone explodes with alerts. ChillSpot's video streaming service just went dark. Fifteen thousand concurrent viewers dropped. The payment processing for premium subscriptions stopped mid-transaction. Revenue is bleeding at $2,000 per minute.

You SSH into the production server, coffee forgotten on your nightstand. The logs tell the story: someone deployed manually this afternoon, skipped the database migration step, and the application has been running on outdated schema for six hours. The evening spike finally broke it.

This exact scenario is why Bash matters.

Not because you need to memorize syntax. Not to write clever one-liners. Because production systems need automation so reliable that deployments run at 3 AM without waking anyone. So repeatable that a junior engineer's first deploy works identically to a senior's hundredth. So transparent that when something does break, you know exactly which step failed and why.

Bash is the language of production automation. Every resilient system you've admired runs on scripts someone wrote, tested, and trusted. Understanding Bash means you can build those systems instead of just using them.

We'll start with what Bash actually is and why it became the backbone of DevOps. Then we'll move through the shell environment, script creation, variables, quoting, special parameters, environment handling, and all the foundational mechanics you need. By the end, you'll understand how Bash thinks, how it executes, and how to write scripts that work reliably when it matters most.

---

## Table of Contents

1. [What Bash Is & Why It Runs Production](#what-bash-is--why-it-runs-production)
2. [The Shell Environment](#the-shell-environment)
3. [Creating Scripts That Work](#creating-scripts-that-work)
4. [Variables: Storing and Using Data](#variables-storing-and-using-data)
5. [Quoting: Getting It Right](#quoting-getting-it-right)
6. [Special Variables & Parameters](#special-variables--parameters)
7. [Environment Variables](#environment-variables)
8. [Parameter Expansion](#parameter-expansion)
9. [String Operations](#string-operations)
10. [Command Substitution](#command-substitution)
11. [Arithmetic Operations](#arithmetic-operations)
12. [Reading User Input](#reading-user-input)
13. [Exit Status: How Success Works](#exit-status-how-success-works)
14. [Real-World Script Patterns](#real-world-script-patterns)

---

<details>
<summary><strong>1. What Bash Is & Why It Runs Production</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Every automation task in DevOps starts somewhere. Most of the time, it starts with Bash.

Deploying code to production? There's a Bash script orchestrating the steps. Backing up databases at 2 AM? Bash coordinates the dump, compression, and S3 upload. Processing application logs to detect anomalies? Bash pipes the data through grep, awk, and your analysis tools. Managing infrastructure provisioning? Even when you're running Terraform or Ansible, Bash scripts often wrap the workflow.

This isn't about Bash being the "best" language for every task. It's about Bash being everywhere when you need it. Every Linux server includes it. Every container has it. Every CI/CD runner provides it. When you need to automate something and installing dependencies requires a security review that takes three weeks, Bash is already there, already approved, already working.

You can automate those repetitive tasks that drain forty minutes from every day. You can write deployment scripts that work reliably instead of "mostly work if you run them carefully." You can debug production issues faster because you understand what the shell is actually doing with your commands. You can build CI/CD pipelines confidently instead of copying Stack Overflow snippets and hoping they work in your context.

---

### What Bash Actually Is

Bash stands for "Bourne Again Shell" — a deliberate pun on the original Bourne shell (sh) that Stephen Bourne created at Bell Labs in 1979. Bash is both an interactive command interpreter and a scripting language, which makes it unusual. The same tool that gives you a prompt for running commands also executes your automation scripts.

When you type `ls -la` in a terminal, Bash interprets that command. When you write a 300-line deployment script, Bash executes that too. Same interpreter, same semantics, same behavior. This duality means everything you learn interactively applies directly to scripting.

Bash became the standard through a combination of timing, licensing, and practical design. It launched in 1989 as free software when commercial Unix shells cost money. It ran on every flavor of Unix and Linux. It stayed backward-compatible with sh while adding features developers actually wanted. And crucially, it became the default shell on Linux distributions during Linux's explosive growth in the late 1990s and early 2000s.

That historical accident matters today. Scripts you write now will run on servers deployed in 2035. Bash takes backward compatibility seriously because breaking existing scripts would break millions of production systems. Your automation doesn't mysteriously stop working when you upgrade from Ubuntu 20.04 to 24.04.

The ecosystem is massive. Every DevOps problem you'll encounter, someone has already solved it in Bash and posted the solution. Stack Overflow has millions of Bash answers. GitHub repositories are full of production-tested scripts. Documentation exists for every edge case. When you get stuck at 2 AM debugging a deployment, that collective knowledge base becomes invaluable.

Bash scales across complexity levels naturally. Need a three-line script to check if a port is open? Done. Need a three-hundred-line script to orchestrate zero-downtime blue-green deployments? Same language, just more structure. You don't switch tools as your needs grow — you apply the same concepts at larger scale.

---

### Where Bash Excels

Bash excels at gluing systems together. You need to run a database backup, compress it, upload to S3, verify the checksum matches, update a status table, and send a Slack notification? Bash handles that orchestration naturally because it was designed specifically for combining Unix tools into workflows.

It's perfect for automating CLI tools, and almost everything in DevOps exposes a command-line interface. Terraform, Docker, kubectl, aws-cli, gcloud, psql, redis-cli — Bash scripts can drive all of them. You're not writing business logic; you're coordinating existing tools, which is exactly what Bash was built for.

It's ideal for system administration tasks that map directly to Unix operations. File manipulation, process management, log processing, user management, permission changes — these operations have corresponding Bash commands that have been refined over decades. The impedance mismatch is minimal.

It's essential for deployment automation in environments where you control the entire stack. Pull code from Git, build artifacts, run database migrations, deploy to servers, verify health checks, update load balancer configuration — Bash ties these steps into a reliable pipeline that can run unattended.

---

### Where Bash Doesn't Make Sense

Complex data processing belongs in Python, awk, or specialized tools. If you're parsing gigabytes of JSON, transforming nested data structures, or performing statistical calculations, reach for a language designed for data manipulation. You can call those tools from Bash, but don't try to do the heavy lifting in Bash itself.

Large-scale applications need proper programming languages with type systems, testing frameworks, and dependency management. If your "script" is three thousand lines with a complex object model, you've outgrown Bash. Use Go, Python, Rust, or another language designed for software engineering at scale.

Anything requiring sophisticated error handling, async operations, or complex state management should use languages built for those patterns. Bash error handling works and can be robust, but it's not as elegant as try-catch-finally in modern languages.

The signal is simple: if you're fighting Bash's limitations, you're using the wrong tool. Bash should feel natural for the task. When it starts feeling awkward, that's the cue to use something else for that piece and let Bash orchestrate it.

---

### Bash vs Other Shells

You'll encounter other shells in production environments. Understanding the differences helps you write portable scripts and choose the right tool.

**sh (POSIX shell)** is the minimal standard that's guaranteed everywhere, including embedded systems, network equipment, and ancient Unix installations. Scripts written for pure sh will run on any Unix-like system, but you lose Bash's arrays, advanced pattern matching, and convenient features. Use sh only when you need maximum portability across heterogeneous systems or when you're writing something that might run on resource-constrained hardware.

**zsh** is feature-rich and increasingly popular for interactive use. It's the default shell on macOS and offers better tab completion, nicer prompts, spelling correction, and extensive customization. For scripts though, stick with Bash — it's more widely available on production servers, and zsh's extensions won't help with automation reliability.

**dash** is lightweight and fast, designed as a minimal POSIX shell. Some Linux distributions use it as `/bin/sh` for boot scripts because it starts faster than Bash. It's POSIX-compliant but lacks Bash extensions. You'll rarely write scripts specifically for dash, but you might encounter it when debugging why a script that works on your laptop fails during system boot.

**fish** is beginner-friendly with excellent interactive features, clean syntax highlighting, helpful suggestions, and sane defaults. But it's intentionally not POSIX-compliant and rarely found on production servers. It's unsuitable for DevOps automation because you can't assume it's installed.

For production automation scripts, use Bash. For your interactive shell on your laptop, use whatever makes you productive. The distinction matters — your personal shell preference doesn't constrain how you write portable automation.

---

### The Scripting Mindset

Learning Bash syntax takes an afternoon. Learning to write reliable scripts takes experience, but that experience can be taught.

Good Bash scripts handle errors explicitly. They check whether commands succeeded before continuing. They fail loudly and immediately when something's wrong instead of silently producing incorrect results that corrupt data or leave systems in inconsistent states. The worst bugs are the ones that look like they worked.

Good Bash scripts are observable. They log what they're doing at appropriate detail levels. They report progress clearly enough that you can tell if a script hung or is just processing slowly. When something breaks at 2 AM and you're reading logs through bleary eyes, clear logging is the difference between a five-minute fix and an hour-long debugging session.

Good Bash scripts are idempotent. Running the script twice produces the same result as running it once. This prevents accidental damage from double-clicks, retry logic, or running the wrong script version. It also makes scripts safer to test because you can run them repeatedly in development without corrupting state.

Good Bash scripts are documented at the right level. The header explains what the script does, how to use it, what arguments it expects, and what exit codes mean. Comments explain why decisions were made, not what the code does — the code itself shows what it does. If you need comments to explain what your code is doing, the code needs to be simpler.

This mindset separates quick hacks from production-ready automation. A quick hack works when you run it carefully. Production automation works when someone else runs it at 3 AM under pressure. We'll build toward this mindset throughout the series, introducing error handling, logging, and reliability patterns as natural extensions of the core concepts.

</div>

</details>

---

<details>
<summary><strong>2. The Shell Environment</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

When you type commands in a terminal, you're not talking directly to the operating system. You're talking to a shell — a program that interprets your commands and tells the OS what to do.

The shell is your interface to everything. It interprets commands. It manages variables. It executes programs. It handles I/O redirection. It runs scripts. Understanding how the shell works helps you write scripts that behave correctly in different contexts.

---

### Interactive vs Non-Interactive Mode

The shell operates in two fundamentally different modes, and this affects how your scripts behave.

**Interactive mode** is when you're typing commands manually. You see a prompt. You type a command. The shell executes it and shows you the result. The shell loads profile configuration files like `.bashrc` and `.bash_profile`. It saves command history. It provides tab completion and helpful features.

**Non-interactive mode** is when scripts run automatically. No prompt appears. The shell doesn't load the same startup files. History isn't saved. Tab completion doesn't exist. This is how cron jobs run, how CI/CD pipelines execute scripts, how systemd services start processes.

Here's why this matters: Scripts that work perfectly when you run them manually can fail when run by cron. Why? Because cron runs scripts non-interactively with a minimal environment. Commands that work in your interactive shell might not be in the PATH. Environment variables you rely on might not be set.

**The lesson:** Write scripts that explicitly set up their environment. Don't assume variables exist. Don't rely on PATH containing custom directories. Don't depend on interactive shell features. Make your scripts work non-interactively from the start.

---

### Login vs Non-Login Shells

Another distinction that affects script behavior: login vs non-login shells.

**Login shells** start when you log into a system via SSH or a physical terminal. They load `/etc/profile`, then `~/.bash_profile` or `~/.profile`. These files set up your environment — PATH, environment variables, aliases, shell options.

**Non-login shells** start when you open a new terminal tab or run `bash` from an existing shell. They load `~/.bashrc` but skip the profile files. Most interactive shells you use daily are non-login shells.

Scripts run as non-login, non-interactive shells by default. They load neither profile files nor rc files unless you explicitly source them.

**Why this matters for ChillSpot:** Imagine you write a deployment script that relies on environment variables set in `~/.bash_profile`. It works when you run it manually because your interactive shell loaded those variables. But when Jenkins runs the script in CI/CD, those variables don't exist. The deployment fails.

Solution: Scripts should define their environment explicitly or source configuration files deliberately. Never rely on implicit environment setup.

---

### How the Shell Executes Commands

When you type a command or run it from a script, the shell follows a specific process:

First, it checks if the command is a shell builtin — commands like `cd`, `echo`, `read` that are part of the shell itself. Builtins execute directly without creating a new process.

Second, it checks if the command is a function defined in the current shell session. Functions run in the current shell's context.

Third, it checks if the command is an alias — a shorthand you've defined for longer commands. Aliases only work in interactive shells, not in scripts (unless you explicitly enable them).

Fourth, it searches the PATH. The shell looks through each directory listed in the PATH environment variable, in order, until it finds an executable file with the matching name.

If none of these steps find the command, you get "command not found."

**This explains common script failures:** You run `npm` or `python` or some custom tool in your script. The script fails with "command not found" even though the command works in your terminal. Why? Because the script's PATH doesn't include the directory where that tool is installed.

**The fix:** Either use absolute paths (`/usr/local/bin/node` instead of `node`) or explicitly set PATH in your script.

---

### Process Creation and Execution

When the shell runs an external command (anything that's not a builtin), it creates a new process through what's called fork-exec. Understanding this pattern explains many behaviors that seem mysterious otherwise.

The shell forks first — it creates a complete copy of itself as a child process. This child process is identical to the parent: same memory, same variables, same file descriptors, same everything. Then the child process calls exec, which replaces the child's memory space with the target program. The child essentially transforms into the new program while keeping the same process ID.

This two-step dance has profound implications. The child process starts with an exact copy of the parent's environment, which is why environment variables propagate to child processes — they're inherited during the fork. But after the fork, the two processes are separate. Changes in the child don't affect the parent. When the child exits, its entire state disappears.

This explains a pattern that confuses people initially: if a script sets a variable and then calls another script, that child script sees the variable (because it inherited the environment). But if the child script modifies the variable, the parent script never sees that change (because the child's environment died when it exited). They're separate processes with separate memory spaces.

**Example at ChillSpot:** You write a deployment script that calls a helper script to detect the current version number. The helper script sets `CURRENT_VERSION=2.3.1`. But when control returns to the main script, `CURRENT_VERSION` is still empty. The helper script set it in its own environment, which disappeared when it exited.

**The solution:** Either source the helper script (`. ./helper.sh` makes it run in the current shell, not as a subprocess) or have the helper script output the value for the parent to capture: `CURRENT_VERSION=$(./helper.sh)`. Sourcing executes the script's commands in your current shell. Capturing output treats the script as a command and grabs what it writes to stdout.

This fork-exec pattern is why Bash can orchestrate programs written in any language. The programs don't need to know they're being called from Bash. They just need to follow Unix conventions: read from stdin, write to stdout, write errors to stderr, return an exit code. Bash handles the rest.

---

### Environment Inheritance

Scripts inherit environment variables from their parent process but start with a fresh set of local variables.

When you run a script, it sees all environment variables (`PATH`, `HOME`, `USER`, `DATABASE_URL`, etc.) from the shell that launched it. But it doesn't see local variables or functions defined in that shell.

This is actually a feature, not a bug. It creates isolation. Scripts can't accidentally depend on variables you set in your interactive session. They must define their dependencies explicitly.

**Best practice:** Treat the script's environment as minimal. Don't assume variables exist. Set up what you need at the top of the script.

</div>

</details>

---

<details>
<summary><strong>3. Creating Scripts That Work</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

You know how to type commands. Now let's make those commands repeatable, reliable, and portable by turning them into scripts.

---

### The Shebang Line

Every Bash script should start with a shebang line — the `#!` at the top that tells the system which interpreter to use.

```bash
#!/bin/bash
```

This line isn't a comment, even though it starts with `#`. When you make a file executable and run it with `./script.sh`, the kernel reads the first two bytes. If they're `#!`, the kernel extracts the rest of the line and uses it as the interpreter.

Without a shebang, the system guesses. On most systems, it defaults to `/bin/sh`, which might not be Bash. Your script might use Bash-specific features that don't work in sh, and suddenly it fails on certain systems.

**Why `/bin/bash` specifically:** This is the standard location for Bash on Linux systems. It works on Ubuntu, CentOS, Debian, and most distributions.

**Alternative for portability:**

```bash
#!/usr/bin/env bash
```

This finds Bash wherever it lives in the system's PATH. It's more portable across different Unix-like systems (BSD, macOS, etc.) where Bash might be installed in `/usr/local/bin/bash` instead of `/bin/bash`.

**Which should you use?** For Linux servers (which is most DevOps work), `#!/bin/bash` is fine and more explicit. For scripts that need to run on macOS or BSD systems, `#!/usr/bin/env bash` is safer.

At ChillSpot, our deployment scripts use `#!/bin/bash` because all our production servers are Ubuntu. Our local development scripts use `#!/usr/bin/env bash` because developers use macOS, Linux, and WSL.

---

### Creating a Script

Let's write a simple script that checks if ChillSpot's Redis cache is running:

```bash
#!/bin/bash
# check_redis.sh - Verify Redis is responsive

redis-cli ping > /dev/null 2>&1

if [ $? -eq 0 ]; then
    echo "Redis is healthy"
    exit 0
else
    echo "Redis is down!"
    exit 1
fi
```

Save this as `check_redis.sh`.

Now make it executable:

```bash
chmod +x check_redis.sh
```

The `chmod +x` command adds execute permission. Without this, you can't run the script with `./check_redis.sh` — you'd have to call `bash check_redis.sh` explicitly, which defeats the purpose of the shebang.

---

### Running Scripts: The Right Way

You've made the script executable. Now you run it:

```bash
./check_redis.sh
```

Notice the `./` prefix. You must specify the path — current directory (`.`), full path (`/home/user/scripts/check_redis.sh`), or relative path (`../scripts/check_redis.sh`).

**Why can't you just type `check_redis.sh`?**

Security. Linux doesn't include the current directory (`.`) in the PATH by default. If it did, malicious users could place a script named `ls` in a shared directory, and when you typed `ls`, you'd run their malicious script instead of the real `/bin/ls` command.

This is actually a good thing. It forces you to be explicit about running local scripts.

**Alternative execution methods:**

```bash
# Explicit interpreter (no execute permission needed)
bash check_redis.sh

# Reading script as input
bash < check_redis.sh

# Sourcing (runs in current shell, not subprocess)
source check_redis.sh
# or
. check_redis.sh
```

The sourcing method is different. When you source a script, it runs in your current shell session instead of creating a subprocess. Variables set in the script remain available after it finishes. Functions defined in the script become available in your shell.

**When to source:** When you want the script to modify your current environment. For example, scripts that set environment variables for development setups or activate Python virtual environments.

**When to execute normally:** For everything else. Most scripts should run in subprocesses to avoid polluting your environment.

---

### Script Headers: Making Scripts Maintainable

Production scripts at ChillSpot all follow a standard header format:

```bash
#!/bin/bash
#
# deploy_api.sh - Deploy ChillSpot streaming API to production
#
# DESCRIPTION:
#   Performs zero-downtime deployment of the streaming API service.
#   Creates backup, updates code, runs migrations, restarts services.
#
# USAGE:
#   deploy_api.sh <environment> <version>
#
# ARGUMENTS:
#   environment    Target environment (staging|production)
#   version        Git tag to deploy (e.g., v2.3.1)
#
# EXAMPLES:
#   deploy_api.sh staging v2.3.1
#   deploy_api.sh production v2.3.0
#
# EXIT CODES:
#   0  Success
#   1  Invalid arguments
#   2  Pre-deployment checks failed
#   3  Deployment failed
#   4  Health check failed
#
# AUTHOR: ChillSpot DevOps Team
# LAST MODIFIED: 2024-11-30

set -euo pipefail  # Strict mode
```

This header tells future you (and your teammates) everything they need to know without reading the entire script. What does it do? How do you use it? What can go wrong? Who wrote it?

**The `set -euo pipefail` line is crucial for production scripts:**

- `set -e` — Exit immediately if any command fails
- `set -u` — Treat undefined variables as errors
- `set -o pipefail` — Pipeline fails if any command in it fails

We'll cover error handling in depth in File 08, but start with strict mode from the beginning. It catches bugs early.

---

### Script Organization

As scripts grow, organization matters. Here's how we structure scripts at ChillSpot:

```bash
#!/bin/bash
set -euo pipefail

# ============================================================================
# Configuration
# ============================================================================

readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly API_DIR="/opt/chillspot/api"
readonly BACKUP_DIR="/var/backups/chillspot"
readonly LOG_FILE="/var/log/chillspot/deploy.log"

readonly MAX_RETRIES=5
readonly HEALTH_CHECK_TIMEOUT=30

# ============================================================================
# Functions
# ============================================================================

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $*" | tee -a "$LOG_FILE"
}

check_prerequisites() {
    # Validation logic here
}

perform_backup() {
    # Backup logic here
}

deploy_code() {
    # Deployment logic here
}

verify_health() {
    # Health check logic here
}

# ============================================================================
# Main Script
# ============================================================================

main() {
    log "Starting deployment..."
    
    check_prerequisites
    perform_backup
    deploy_code
    verify_health
    
    log "Deployment complete!"
}

# Run main function
main "$@"
```

This structure scales well. Configuration at the top is easy to find and modify. Functions are defined before use. The main logic is clear and readable. Everything is logged.

The `main "$@"` pattern at the end is a best practice. It passes all script arguments (`"$@"`) to the main function, making argument handling consistent and testable.

---

### Making Scripts Portable

Scripts that work on your laptop need to work on production servers. Here's how to make them portable:

**Use absolute paths for critical files:**

```bash
# Bad - relies on current directory
./config.yml

# Good - explicit path
/etc/chillspot/config.yml
```

**Find the script's directory reliably:**

```bash
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
```

Now you can reference files relative to the script's location:

```bash
source "${SCRIPT_DIR}/lib/common.sh"
```

**Check for required commands:**

```bash
for cmd in redis-cli jq curl; do
    if ! command -v "$cmd" &> /dev/null; then
        echo "Error: Required command '$cmd' not found"
        exit 1
    fi
done
```

**Set up PATH explicitly if needed:**

```bash
export PATH="/usr/local/bin:/usr/bin:/bin"
```

This ensures the script behaves consistently regardless of who runs it or from where.

</div>

</details>

---

<details>
<summary><strong>4. Variables: Storing and Using Data</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Variables store information your scripts need to operate. Version numbers. File paths. API endpoints. User input. Without variables, you'd have to hardcode everything, making scripts inflexible and unmaintainable.

---

### Assigning Variables

Variable assignment in Bash is simple but strict:

```bash
service_name="chillspot-api"
version="2.3.1"
port=8080
database_host="db.chillspot.internal"
```

**Critical rule:** No spaces around the equals sign.

This fails:

```bash
service_name = "chillspot-api"  # Error: command not found
```

Why? Bash interprets this as running a command called `service_name` with arguments `=` and `"chillspot-api"`. It doesn't see an assignment — it sees a command execution.

The no-spaces rule is non-negotiable. It catches everyone at first. You'll forget it. Your script will fail with a confusing error. You'll remember.

---

### Accessing Variables

You reference variables with a dollar sign:

```bash
api_version="2.3.1"

echo $api_version           # Simple access
echo ${api_version}         # Explicit braces
echo "$api_version"         # Quoted (safe)
echo 'Version: $api_version'  # Literal (no expansion)
```

**Always quote your variables in practice:**

```bash
# Dangerous - breaks with spaces or special characters
file_path=/tmp/my document.txt
rm $file_path  # Tries to delete "/tmp/my" and "document.txt"

# Safe - treats as single argument
file_path="/tmp/my document.txt"
rm "$file_path"  # Deletes "/tmp/my document.txt"
```

Unquoted variables undergo word splitting. The shell splits on whitespace and treats each piece as a separate argument. This causes subtle bugs that only appear when filenames have spaces, which happens more often in production than you'd expect.

**ChillSpot example:** Our video processing scripts handle user-uploaded files. Users name files things like "vacation 2024.mp4". If we don't quote variables, those spaces cause commands to fail with cryptic errors.

---

### Variable Naming Conventions

Consistent naming makes scripts readable:

| Type | Convention | Example |
|------|------------|---------|
| Local variables | lowercase_with_underscores | `backup_dir="/var/backups"` |
| Constants | UPPERCASE_WITH_UNDERSCORES | `readonly MAX_RETRIES=5` |
| Environment variables | UPPERCASE_WITH_UNDERSCORES | `export DATABASE_URL="..."` |
| Functions | lowercase_with_underscores | `function check_health() { ... }` |

At ChillSpot, our scripts follow this religiously:

```bash
#!/bin/bash

# Constants (never change)
readonly CONFIG_FILE="/etc/chillspot/api.conf"
readonly MAX_UPLOAD_SIZE=104857600  # 100MB

# Configuration (set once, used throughout)
environment="production"
api_version="2.3.1"

# Working variables (change during execution)
current_status=""
retry_count=0
deployment_timestamp=""

# Environment variables (passed to subprocesses)
export CHILLSPOT_ENV="production"
export DATABASE_URL="postgresql://db.chillspot.internal/streaming"
```

This pattern makes it immediately clear what each variable represents and how it should be used.

---

### Using Braces for Clarity

Braces `${}` disambiguate variable names from surrounding text:

```bash
# Without braces - ambiguous
environment=production
echo $environment_database  # Looks for variable $environment_database

# With braces - explicit
echo ${environment}_database  # production_database
```

You need braces when:
- The variable name is followed by characters that could be part of the name
- You're doing parameter expansion operations
- You want to be explicit about what's a variable

At ChillSpot, we construct URLs dynamically:

```bash
environment="staging"
region="us-east-1"

# Braces make this clear
api_url="https://api-${environment}.chillspot.com"
bucket_name="chillspot-${environment}-${region}-videos"
```

Without braces, `$environment-staging` would look for a variable called `environment-staging`.

---

### Constants with readonly

Some values shouldn't change during script execution. Configuration paths. Retry limits. API endpoints. Make these readonly:

```bash
readonly MAX_RETRIES=5
readonly API_ENDPOINT="https://api.chillspot.com"
readonly LOG_DIR="/var/log/chillspot"

# Attempting to change causes error
MAX_RETRIES=10  # Error: MAX_RETRIES: readonly variable
```

This prevents accidental modification in complex scripts. If you have 500 lines of code, you don't want to accidentally reuse `MAX_RETRIES` as a loop counter and break your retry logic.

Constants also serve as documentation. When someone reads your script, `readonly CONFIG_FILE` immediately signals "this path is fixed, don't try to change it dynamically."

---

### Variable Scope

Variables exist either globally (throughout the entire script) or locally (only within functions).

```bash
#!/bin/bash

# Global variable - available everywhere
deployment_id="deploy_20241130_153045"

deploy_service() {
    # Local variable - only exists in this function
    local service_name="chillspot-api"
    local service_port=8080
    
    echo "Deploying $service_name on port $service_port"
    echo "Deployment ID: $deployment_id"  # Can access global
}

deploy_service
echo "Service: $service_name"  # Empty - service_name was local
echo "Deployment: $deployment_id"  # Works - deployment_id is global
```

**Best practice:** Always use `local` for variables inside functions. This prevents functions from accidentally modifying global state.

At ChillSpot, we learned this the hard way. An engineer wrote a helper function that set a variable called `version`. Three hundred lines later, the main script set a global variable also called `version`. The function ran, overwrote the global, and the deployment used the wrong version. Everything broke.

Now we enforce `local` in functions. No exceptions.

---

### Unsetting Variables

Sometimes you need to remove a variable entirely:

```bash
api_token="secret_token_here"

# Use the token
curl -H "Authorization: Bearer $api_token" https://api.chillspot.com/videos

# Clear it from memory
unset api_token

# Now it's gone
echo "$api_token"  # Empty
```

This matters for security. If your script handles sensitive data (passwords, tokens, API keys), unset those variables after use. It won't stop a determined attacker who has access to the running process, but it reduces the window of exposure.

</div>

</details>

---

<details>
<summary><strong>5. Quoting: Getting It Right</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Quoting controls how Bash interprets strings and variables. Get it wrong and your scripts break in subtle ways. Get it right and your scripts handle edge cases gracefully.

---

### Single Quotes: Everything is Literal

Single quotes preserve everything literally. No variable expansion. No command substitution. No escape sequences. What you type is exactly what you get.

```bash
message='Hello $USER, today is $(date)'
echo $message
# Output: Hello $USER, today is $(date)
```

The `$USER` variable isn't expanded. The `$(date)` command isn't executed. It's all literal text.

**Use single quotes when:**
- You're working with literal strings that contain special characters
- You're storing text that might contain `$`, backticks, or other special characters
- You're debugging and want to see exact values without expansion

At ChillSpot, we use single quotes for regex patterns:

```bash
# Search logs for errors
grep 'ERROR: .* failed' /var/log/chillspot/api.log

# The .* is literal - we want grep to interpret it, not Bash
```

---

### Double Quotes: Controlled Expansion

Double quotes allow variable expansion and command substitution but prevent word splitting and globbing:

```bash
user="alice"
date_str="$(date +%Y-%m-%d)"

message="Hello $user, today is $date_str"
echo "$message"
# Output: Hello alice, today is 2024-11-30
```

Notice how the variables expanded and the command ran, but the result is still treated as a single cohesive string.

**Double quotes prevent word splitting:**

```bash
file_name="vacation videos 2024.mp4"

# Without quotes - disaster
ls $file_name
# Bash splits on spaces: tries to ls three separate things: "vacation", "videos", "2024.mp4"
# Error: cannot access 'vacation': No such file or directory

# With quotes - correct behavior
ls "$file_name"
# Treated as single argument: correctly accesses "vacation videos 2024.mp4"
```

This word-splitting behavior catches people constantly. The variable contains one filename with spaces, but without quotes, Bash sees three separate arguments. This is why the rule "always quote your variables" exists — even if you think a variable will never contain spaces, you don't control what values might end up in it.

At ChillSpot, user-uploaded video titles can contain anything imaginable. Spaces, Unicode characters, emoji, punctuation. Our scripts quote every variable reference to avoid breaking on unexpected input. The habit prevents entire categories of bugs.

---

### No Quotes: Dangerous Territory

Unquoted strings undergo:
- Parameter expansion (variables get replaced)
- Command substitution (commands get executed)
- Word splitting (spaces create separate arguments)
- Pathname expansion (globs like `*` and `?` expand)

```bash
files="*.txt"

# Unquoted - glob expands
echo $files
# Output: file1.txt file2.txt file3.txt

# Quoted - literal
echo "$files"
# Output: *.txt
```

Sometimes you want globbing. Most of the time you don't. Quote by default, unquote deliberately.

---

### Escaping Special Characters

Backslash escapes single characters:

```bash
# Escape a dollar sign
price="\$9.99"
echo $price  # $9.99

# Escape a newline to continue line
long_command="curl https://api.chillspot.com/videos \
    -H 'Authorization: Bearer $TOKEN' \
    -H 'Content-Type: application/json'"
```

Inside double quotes, backslash escapes `$`, backticks, `"`, `\`, and newlines. Other characters don't need escaping.

---

### Quoting Best Practices

At ChillSpot, these rules govern our scripts:

**Always quote variable expansions:**

```bash
# Yes
echo "$user_name"
rm "$file_path"
curl "$api_url"

# No
echo $user_name    # Unless you want word splitting
rm $file_path      # Dangerous with spaces
curl $api_url      # Breaks on special characters
```

**Use single quotes for literal strings:**

```bash
# Yes
echo 'Deployment failed - check $LOG_FILE'  # Want literal $LOG_FILE

# No
echo "Deployment failed - check $LOG_FILE"  # Would expand variable
```

**Use double quotes when you need expansion:**

```bash
# Yes
echo "Deploying version $version to $environment"

# No
echo 'Deploying version $version to $environment'  # Wouldn't expand
```

**Quote entire command substitutions:**

```bash
# Yes
current_date="$(date +%Y-%m-%d)"

# No
current_date=$(date +%Y-%m-%d)  # Works but inconsistent
```

Consistent quoting makes scripts predictable and maintainable.

</div>

</details>

---

<details>
<summary><strong>6. Special Variables & Parameters</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Bash provides special variables that give you information about the script's execution context and arguments.

---

### Positional Parameters

When you run a script with arguments, Bash stores them in numbered variables:

```bash
# deploy.sh production v2.3.1
# $0 = ./deploy.sh
# $1 = production
# $2 = v2.3.1
```

Access them directly:

```bash
#!/bin/bash

environment=$1
version=$2

echo "Deploying version $version to $environment"
```

Run it:

```bash
./deploy.sh staging v2.3.1
# Output: Deploying version v2.3.1 to staging
```

**Problem:** What if someone runs it without arguments? Your script operates on empty variables, which causes cryptic errors.

**Solution:** Validate arguments:

```bash
#!/bin/bash

if [ $# -lt 2 ]; then
    echo "Usage: $0 <environment> <version>"
    echo "Example: $0 production v2.3.1"
    exit 1
fi

environment=$1
version=$2

# Now you know arguments exist
echo "Deploying version $version to $environment"
```

The `$#` variable holds the count of arguments passed. Checking it first prevents operating on missing data.

---

### Special Parameter Reference

| Variable | Meaning | Example |
|----------|---------|---------|
| `$0` | Script name | `./deploy.sh` |
| `$1`, `$2`, ... `$9` | Positional arguments | `production`, `v2.3.1` |
| `${10}`, `${11}`, ... | Arguments beyond 9 (need braces) | `${10}` |
| `$#` | Number of arguments | `2` |
| `$@` | All arguments as separate words | `"production" "v2.3.1"` |
| `$*` | All arguments as single string | `"production v2.3.1"` |
| `$$` | Current process ID | `12345` |
| `$?` | Exit status of last command | `0` (success), `1` (failure) |
| `$!` | PID of last background job | `12346` |
| `$_` | Last argument of previous command | `v2.3.1` |

---

### The `$@` vs `$*` Distinction

Both represent all arguments, but they behave differently when quoted:

```bash
#!/bin/bash

echo "Using \$*:"
for arg in "$*"; do
    echo "  - $arg"
done

echo "Using \$@:"
for arg in "$@"; do
    echo "  - $arg"
done
```

Run it:

```bash
./test.sh one "two three" four
```

Output:

```
Using $*:
  - one two three four

Using $@:
  - one
  - two three
  - four
```

`"$*"` combines all arguments into a single string. `"$@"` preserves each argument as a separate word, maintaining quotes.

**Use `"$@"` when passing arguments to other commands.** It preserves the structure correctly.

At ChillSpot, our deployment wrapper script passes arguments to the actual deployment script:

```bash
#!/bin/bash
# deploy_wrapper.sh - Adds logging and notifications

echo "Starting deployment at $(date)"
./deploy.sh "$@"  # Passes all arguments correctly
echo "Deployment finished at $(date)"
```

This works because `"$@"` maintains argument boundaries.

---

### Exit Status: `$?`

Every command returns an exit code. By convention:
- `0` means success
- Non-zero means failure (specific meaning varies)

Check the exit status of the last command:

```bash
redis-cli ping > /dev/null

if [ $? -eq 0 ]; then
    echo "Redis is up"
else
    echo "Redis is down"
fi
```

**Better pattern using the command directly:**

```bash
if redis-cli ping > /dev/null; then
    echo "Redis is up"
else
    echo "Redis is down"
fi
```

This is cleaner because `if` tests the exit status directly. No need to check `$?`.

At ChillSpot, our health check scripts test services this way:

```bash
#!/bin/bash

# Check Redis
if redis-cli ping > /dev/null 2>&1; then
    echo "✓ Redis is healthy"
else
    echo "✗ Redis is down"
    exit 1
fi

# Check PostgreSQL
if pg_isready -q; then
    echo "✓ PostgreSQL is healthy"
else
    echo "✗ PostgreSQL is down"
    exit 1
fi

# Check API
if curl -sf http://localhost:8080/health > /dev/null; then
    echo "✓ API is healthy"
else
    echo "✗ API is down"
    exit 1
fi

echo "All services healthy"
exit 0
```

Each check returns an appropriate exit code. The script exits early if any check fails.

---

### Process ID: `$$`

The current script's process ID is available in `$$`. This is useful for creating unique temporary files or lock files:

```bash
# Create temporary file unique to this script run
temp_file="/tmp/deploy_$$.tmp"
echo "Working..." > "$temp_file"

# Clean up when done
rm "$temp_file"
```

At ChillSpot, our deployment scripts create lock files to prevent concurrent deployments:

```bash
lock_file="/var/run/chillspot_deploy.lock"

if [ -f "$lock_file" ]; then
    echo "Deployment already in progress (PID $(cat $lock_file))"
    exit 1
fi

# Create lock file with our PID
echo $$ > "$lock_file"

# Ensure lock file is removed on exit
trap "rm -f $lock_file" EXIT

# Deployment happens here...
```

This prevents two engineers from accidentally deploying simultaneously.

---

### Shifting Arguments

The `shift` command moves positional parameters down by one:

```bash
#!/bin/bash

echo "First argument: $1"
shift
echo "Now first argument is: $1"
shift
echo "Now first argument is: $1"
```

Run it:

```bash
./test.sh one two three
# Output:
# First argument: one
# Now first argument is: two
# Now first argument is: three
```

This is useful when processing flags before arguments:

```bash
#!/bin/bash

verbose=0

while [ $# -gt 0 ]; do
    case $1 in
        -v|--verbose)
            verbose=1
            shift
            ;;
        *)
            break
            ;;
    esac
done

# Remaining arguments are actual parameters
environment=$1
version=$2
```

Now the script handles flags cleanly:

```bash
./deploy.sh --verbose production v2.3.1
```

</div>

</details>

---

<details>
<summary><strong>7. Environment Variables</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Environment variables are shared with child processes. When your script runs a command, that command inherits all environment variables from your script.

---

### Built-in Environment Variables

The shell provides many environment variables automatically:

| Variable | Purpose | Typical Value |
|----------|---------|---------------|
| `HOME` | User's home directory | `/home/alice` |
| `USER` | Current username | `alice` |
| `PATH` | Command search path | `/usr/local/bin:/usr/bin:/bin` |
| `PWD` | Current directory | `/opt/chillspot/scripts` |
| `SHELL` | User's default shell | `/bin/bash` |
| `HOSTNAME` | System hostname | `api-server-01` |
| `LANG` | Locale setting | `en_US.UTF-8` |
| `TERM` | Terminal type | `xterm-256color` |

These are already set when your script runs. You can use them directly:

```bash
echo "Running as $USER on $HOSTNAME"
echo "Working directory: $PWD"
```

---

### Creating Environment Variables

Regular variables exist only in the current script. Environment variables are exported to child processes:

```bash
# Regular variable - not inherited
database_name="chillspot_production"

# Environment variable - inherited by subprocesses
export DATABASE_URL="postgresql://localhost/chillspot_production"
export CHILLSPOT_ENV="production"
```

Now when your script runs other commands, they see those environment variables:

```bash
#!/bin/bash

export DATABASE_URL="postgresql://db.chillspot.internal/streaming"
export REDIS_URL="redis://cache.chillspot.internal:6379"

# This Python script will see DATABASE_URL and REDIS_URL
python3 migrate_database.py
```

At ChillSpot, we set environment variables in deployment scripts so our application code can access configuration:

```bash
#!/bin/bash
# start_api.sh

export CHILLSPOT_ENV="production"
export DATABASE_URL="postgresql://db-01.chillspot.internal/streaming"
export REDIS_URL="redis://cache-01.chillspot.internal:6379"
export S3_BUCKET="chillspot-production-videos"
export LOG_LEVEL="info"

# Start the API with environment configured
node server.js
```

The Node.js application reads these via `process.env.DATABASE_URL`, etc.

---

### Environment Variable Scope

Environment variables follow these rules:

**Created in parent, visible in child:**

```bash
export API_KEY="secret123"
./child_script.sh  # Can see API_KEY
```

**Created in child, NOT visible in parent:**

```bash
#!/bin/bash
# parent.sh

./child.sh  # This sets CHILD_VAR=value

echo $CHILD_VAR  # Empty - variable set in child doesn't affect parent
```

This is process isolation working as designed. Each process has its own environment. Changes in child processes don't affect the parent.

**Sourcing makes variables stay:**

```bash
source ./set_env.sh  # Runs in current shell
echo $CHILLSPOT_ENV  # Now visible
```

When you source a script, it runs in the current shell, so variables it sets remain.

---

### Checking if Variables Are Set

Before using environment variables, verify they exist:

```bash
if [ -z "$DATABASE_URL" ]; then
    echo "Error: DATABASE_URL not set"
    exit 1
fi
```

The `-z` test checks if a variable is empty (zero length).

At ChillSpot, our application startup scripts validate required environment variables:

```bash
#!/bin/bash

required_vars=(
    "DATABASE_URL"
    "REDIS_URL"
    "S3_BUCKET"
    "JWT_SECRET"
)

missing_vars=()

for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        missing_vars+=("$var")
    fi
done

if [ ${#missing_vars[@]} -gt 0 ]; then
    echo "Error: Missing required environment variables:"
    printf '  - %s\n' "${missing_vars[@]}"
    exit 1
fi

# All required variables are set - safe to start
node server.js
```

This prevents the application from starting with incomplete configuration.

---

### Default Values

You can provide fallback values if environment variables aren't set:

```bash
# If LOG_LEVEL isn't set, default to "info"
log_level="${LOG_LEVEL:-info}"

# If PORT isn't set, default to 8080
port="${PORT:-8080}"
```

This uses parameter expansion syntax (covered next section). The `:-` operator means "use this default if variable is unset or empty."

At ChillSpot, we use this pattern for optional configuration:

```bash
# Required - no default
database_url="${DATABASE_URL:?DATABASE_URL must be set}"

# Optional - has default
log_level="${LOG_LEVEL:-info}"
worker_count="${WORKER_COUNT:-4}"
request_timeout="${REQUEST_TIMEOUT:-30}"
```

The `:?` operator exits with an error if the variable is unset. The `:-` operator provides a default.

</div>

</details>

---

<details>
<summary><strong>8. Parameter Expansion</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Parameter expansion lets you manipulate variables inline without calling external commands. It's faster and cleaner than using `sed` or `awk` for simple operations.

---

### Default Values

Provide defaults when variables might be unset:

```bash
# If environment isn't set, use "development"
environment="${ENVIRONMENT:-development}"

# If max_retries isn't set, use 5
max_retries="${MAX_RETRIES:-5}"
```

This is defensive programming. Your script works even if optional variables aren't configured.

**The `:=` variant assigns the default back to the variable:**

```bash
# If LOG_DIR is unset, set it to /var/log and use that value
log_dir="${LOG_DIR:=/var/log/chillspot}"
```

Now `LOG_DIR` is set for the rest of the script.

---

### Required Variables

Fail explicitly if required variables are missing:

```bash
database_url="${DATABASE_URL:?DATABASE_URL must be set}"
api_key="${API_KEY:?API_KEY is required}"
```

If the variable is unset, the script exits with the error message. This is better than proceeding with empty values and failing mysteriously later.

At ChillSpot:

```bash
#!/bin/bash

# Required configuration
environment="${ENVIRONMENT:?ENVIRONMENT must be set (staging|production)}"
version="${VERSION:?VERSION must be set (e.g., v2.3.1)}"

echo "Deploying $version to $environment"
```

Run without variables:

```bash
./deploy.sh
# Error: ENVIRONMENT must be set (staging|production)
```

The script fails immediately with a clear message.

---

### String Length

Get the length of a variable's value:

```bash
password="secret123"
length=${#password}  # 9
```

At ChillSpot, we validate minimum password lengths:

```bash
if [ ${#password} -lt 12 ]; then
    echo "Password must be at least 12 characters"
    exit 1
fi
```

---

### Substring Extraction

Extract parts of strings:

```bash
version="v2.3.1"

# Remove first character
echo "${version:1}"      # 2.3.1

# Get first 3 characters
echo "${version:0:3}"    # v2.

# Get everything from position 2
echo "${version:2}"      # 3.1
```

The syntax is `${variable:offset:length}`. Offset starts at 0. Length is optional.

At ChillSpot, we extract major versions from full version strings:

```bash
full_version="v2.3.1"
# Remove the "v" prefix
version_number="${full_version:1}"    # 2.3.1

# Extract just major version
major_version="${version_number:0:1}"  # 2
```

---

### Pattern Removal

Remove patterns from the beginning or end of strings:

```bash
filename="video.mp4"

# Remove shortest match from end
echo "${filename%.mp4}"     # video

# Remove longest match from end
echo "${filename%%.*}"      # video (even if multiple dots)

# Remove shortest match from beginning
path="/opt/chillspot/videos/vacation.mp4"
echo "${path#*/}"           # opt/chillspot/videos/vacation.mp4

# Remove longest match from beginning
echo "${path##*/}"          # vacation.mp4 (basename)
```

**Pattern removal operators:**
- `#` removes shortest match from beginning
- `##` removes longest match from beginning
- `%` removes shortest match from end
- `%%` removes longest match from end

At ChillSpot, we extract filenames and extensions:

```bash
upload_path="/tmp/uploads/user_video_2024.mp4"

# Get filename without path
filename="${upload_path##*/}"     # user_video_2024.mp4

# Get filename without extension
name="${filename%.*}"              # user_video_2024

# Get extension
extension="${filename##*.}"        # mp4

# Get directory path
directory="${upload_path%/*}"      # /tmp/uploads
```

This avoids calling external commands like `basename` and `dirname`.

---

### Pattern Substitution

Replace patterns within strings:

```bash
text="Hello World World"

# Replace first occurrence
echo "${text/World/Universe}"     # Hello Universe World

# Replace all occurrences
echo "${text//World/Universe}"    # Hello Universe Universe

# Replace at beginning
echo "${text/#Hello/Goodbye}"     # Goodbye World World

# Replace at end
echo "${text/%World/Universe}"    # Hello World Universe
```

At ChillSpot, we sanitize user input:

```bash
# User uploaded: "My Vacation Video (2024).mp4"
upload_name="My Vacation Video (2024).mp4"

# Remove parentheses and spaces for filesystem safety
safe_name="${upload_name//[() ]/_}"  # My_Vacation_Video__2024_.mp4
```

---

### Case Conversion

Change case of strings (Bash 4.0+):

```bash
environment="PRODUCTION"

# Convert to lowercase
echo "${environment,,}"           # production

# Convert to uppercase  
echo "${environment^^}"           # PRODUCTION

# Capitalize first letter
name="alice"
echo "${name^}"                   # Alice
```

At ChillSpot, we normalize environment names:

```bash
# User might type: Production, PRODUCTION, production
input="Production"
environment="${input,,}"  # Normalize to lowercase

if [ "$environment" != "production" ] && [ "$environment" != "staging" ]; then
    echo "Invalid environment: $environment"
    exit 1
fi
```

---

### Combining Expansions

You can chain parameter expansions:

```bash
# Get filename, remove extension, convert to lowercase
upload="User_Video.MP4"
name="${${upload##*/}%.*}"  # Won't work - can't nest directly

# Do it in steps
filename="${upload##*/}"     # User_Video.MP4
name="${filename%.*}"        # User_Video
name_lower="${name,,}"       # user_video
```

Or use intermediate variables for readability.

</div>

</details>

---

<details>
<summary><strong>9. String Operations</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Beyond parameter expansion, Bash offers various ways to manipulate and test strings.

---

### Concatenation

Join strings by placing them adjacent:

```bash
first="Hello"
last="World"

# Simple concatenation
greeting="$first $last"         # Hello World

# Building paths
base_dir="/opt/chillspot"
component="api"
full_path="$base_dir/$component"  # /opt/chillspot/api
```

At ChillSpot, we build file paths dynamically:

```bash
environment="production"
region="us-east-1"
service="video-processor"

# Build S3 path
s3_bucket="chillspot-${environment}-${region}-${service}"
# Result: chillspot-production-us-east-1-video-processor
```

---

### String Comparison

Compare strings for equality or ordering:

```bash
if [ "$environment" = "production" ]; then
    echo "Running in production"
fi

if [ "$environment" != "development" ]; then
    echo "Not in development"
fi
```

**Use `=` for equality, not `==`** in `[ ]` tests. The double-equals works in `[[ ]]` but not in `[ ]`. Be consistent.

**String ordering:**

```bash
if [[ "$version1" < "$version2" ]]; then
    echo "$version1 is earlier than $version2"
fi
```

This does lexicographic comparison. It works for version strings like "v2.3.1" and "v2.4.0" only if they have consistent formatting.

---

### Pattern Matching

Check if strings match patterns:

```bash
filename="video.mp4"

# Using case statement
case "$filename" in
    *.mp4)
        echo "MP4 video"
        ;;
    *.mov)
        echo "MOV video"
        ;;
    *)
        echo "Unknown format"
        ;;
esac

# Using [[ ]] pattern matching
if [[ "$filename" == *.mp4 ]]; then
    echo "MP4 file"
fi
```

At ChillSpot, we validate file uploads by extension:

```bash
uploaded_file="$1"

case "$uploaded_file" in
    *.mp4|*.mov|*.avi|*.mkv)
        echo "Valid video format"
        ;;
    *)
        echo "Unsupported format: must be mp4, mov, avi, or mkv"
        exit 1
        ;;
esac
```

---

### Regex Matching

Use `[[ ]]` with `=~` for regex:

```bash
version="v2.3.1"

if [[ "$version" =~ ^v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
    echo "Valid semantic version"
else
    echo "Invalid version format"
    exit 1
fi
```

At ChillSpot, we validate user input with regex:

```bash
email="user@example.com"

if [[ "$email" =~ ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$ ]]; then
    echo "Valid email"
else
    echo "Invalid email format"
    exit 1
fi
```

Regex in Bash uses POSIX Extended Regular Expressions (ERE). No need to escape `+`, `?`, `{`, `}`, `(`, `)` — they're already special.

---

### String Trimming

Remove leading/trailing whitespace:

```bash
# Using parameter expansion
text="  hello  "
trimmed="${text#"${text%%[![:space:]]*}"}"  # Remove leading
trimmed="${trimmed%"${trimmed##*[![:space:]]}"}"  # Remove trailing
```

This is cumbersome. For readability, consider using external tools:

```bash
# Using xargs (works on most systems)
text="  hello  "
trimmed=$(echo "$text" | xargs)

# Using sed
trimmed=$(echo "$text" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
```

At ChillSpot, we trim user input:

```bash
# User input might have accidental spaces
read -p "Enter environment: " environment
environment=$(echo "$environment" | xargs)  # Trim whitespace

if [ "$environment" = "production" ]; then
    echo "Deploying to production"
fi
```

---

### Checking for Substrings

Test if a string contains another string:

```bash
log_line="ERROR: Database connection failed"

if [[ "$log_line" == *ERROR* ]]; then
    echo "Found error in log"
fi
```

The `*ERROR*` pattern matches any string containing "ERROR" anywhere.

At ChillSpot, we parse logs:

```bash
while read -r line; do
    if [[ "$line" == *ERROR* ]] || [[ "$line" == *FATAL* ]]; then
        echo "Critical issue: $line"
    fi
done < /var/log/chillspot/api.log
```

</div>

</details>

---

<details>
<summary><strong>10. Command Substitution</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Command substitution runs a command and captures its output as a string. This lets you use command results as variable values.

---

### Modern Syntax

Use `$(command)` for command substitution:

```bash
current_date=$(date +%Y-%m-%d)
echo "Today is $current_date"

# Count files
file_count=$(ls -1 | wc -l)
echo "Directory contains $file_count files"

# Get current user
username=$(whoami)
echo "Running as $username"
```

At ChillSpot, we use command substitution everywhere:

```bash
# Get current API version
current_version=$(curl -s http://localhost:8080/version | jq -r '.version')

# Count active connections
connection_count=$(netstat -an | grep :8080 | grep ESTABLISHED | wc -l)

# Get database size
db_size=$(psql -t -c "SELECT pg_database_size('chillspot_production')")
```

---

### Legacy Syntax (Avoid)

Backticks also work but are harder to read and nest:

```bash
# Old way - harder to read
current_date=`date +%Y-%m-%d`

# New way - clearer
current_date=$(date +%Y-%m-%d)
```

**Always use `$(...)` instead of backticks.** It's more readable, easier to nest, and clearer when reading code.

---

### Nesting Command Substitution

`$(...)` syntax nests cleanly:

```bash
# Get the directory containing the currently running script
script_dir=$(dirname "$(readlink -f "$0")")

# Count processes owned by current user
my_process_count=$(ps aux | grep "^$(whoami)" | wc -l)
```

With backticks, nesting requires escaping and becomes unreadable. With `$(...)`, it's straightforward.

---

### Capturing Multi-line Output

Command substitution preserves newlines but not trailing ones:

```bash
# Get list of active services
services=$(systemctl list-units --type=service --state=running --no-pager)

# Process line by line
while IFS= read -r line; do
    echo "Service: $line"
done <<< "$services"
```

At ChillSpot, we process command output:

```bash
# Get list of deployed versions
deployed_versions=$(ls -1 /opt/chillspot/releases/)

for version in $deployed_versions; do
    echo "Found deployment: $version"
done
```

---

### Error Handling with Command Substitution

Command substitution captures stdout only. Errors go to stderr:

```bash
# This captures output
result=$(ls /nonexistent 2>&1)

# Without 2>&1, errors appear on terminal
result=$(ls /nonexistent)  # Error message prints, variable is empty
```

At ChillSpot, we capture both output and errors:

```bash
# Try to get API health, capture any errors
health_output=$(curl -sf http://localhost:8080/health 2>&1)

if [ $? -eq 0 ]; then
    echo "API is healthy: $health_output"
else
    echo "API health check failed: $health_output"
    exit 1
fi
```

---

### Performance Considerations

Command substitution creates a subshell. For simple operations, use parameter expansion instead:

```bash
# Slow - external command
filename=$(basename "$filepath")

# Fast - parameter expansion
filename="${filepath##*/}"

# Slow - external command
extension=$(echo "$filename" | awk -F. '{print $NF}')

# Fast - parameter expansion
extension="${filename##*.}"
```

Reserve command substitution for operations that genuinely need external commands. For string manipulation, use parameter expansion.

</div>

</details>

---

<details>
<summary><strong>11. Arithmetic Operations</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Bash can perform integer arithmetic natively. For floating-point math, you need external tools.

---

### Arithmetic Expansion

Use `$((...))` for calculations:

```bash
# Basic operations
sum=$((5 + 3))           # 8
difference=$((10 - 4))   # 6
product=$((6 * 7))       # 42
quotient=$((20 / 4))     # 5
remainder=$((17 % 5))    # 2

# Variables work without $
count=10
new_count=$((count + 1))  # 11
doubled=$((count * 2))    # 20
```

At ChillSpot, we use this for counters and calculations:

```bash
# Retry logic
max_retries=5
attempt=0

while [ $attempt -lt $max_retries ]; do
    if curl -sf http://localhost:8080/health > /dev/null; then
        echo "API is healthy"
        break
    fi
    
    attempt=$((attempt + 1))
    echo "Attempt $attempt of $max_retries failed"
    sleep 2
done
```

---

### Compound Operations

Modify variables in place:

```bash
count=10

# Increment
((count++))        # count is now 11
((count+=5))       # count is now 16

# Decrement
((count--))        # count is now 15
((count-=3))       # count is now 12

# Multiply/divide
((count*=2))       # count is now 24
((count/=4))       # count is now 6
```

At ChillSpot, we track deployment statistics:

```bash
success_count=0
failure_count=0

deploy_service() {
    if ./deploy.sh "$1"; then
        ((success_count++))
    else
        ((failure_count++))
    fi
}

deploy_service "api"
deploy_service "worker"
deploy_service "scheduler"

echo "Deployments: $success_count successful, $failure_count failed"
```

---

### Arithmetic Comparisons

Use `((...))` for numeric comparisons in conditions:

```bash
if ((count > 10)); then
    echo "Count is greater than 10"
fi

if ((attempt < max_retries)); then
    echo "Can retry"
fi

if ((result == 0)); then
    echo "Result is zero"
fi
```

This is cleaner than using `[ ]` tests for numbers.

---

### Floating-Point Math

Bash only handles integers. For decimals, use `bc`:

```bash
# Calculate percentage
total=150
completed=45
percentage=$(echo "scale=2; $completed * 100 / $total" | bc)
echo "Progress: $percentage%"

# Complex calculations
result=$(echo "scale=4; (10.5 * 3.7) / 2.1" | bc)
```

At ChillSpot, we calculate video compression ratios:

```bash
original_size=104857600  # 100MB
compressed_size=31457280  # 30MB

ratio=$(echo "scale=2; $compressed_size * 100 / $original_size" | bc)
echo "Compressed to $ratio% of original size"
```

---

### Random Numbers

Generate random numbers with `$RANDOM`:

```bash
# Random number between 0 and 32767
random_number=$RANDOM

# Random number in specific range (1-100)
random_1_100=$(( (RANDOM % 100) + 1 ))

# Random delay (useful for preventing thundering herd)
sleep_time=$(( (RANDOM % 10) + 1 ))
sleep $sleep_time
```

At ChillSpot, we add jitter to retry delays:

```bash
base_delay=5
jitter=$(( RANDOM % 3 ))  # 0-2 seconds
total_delay=$((base_delay + jitter))

echo "Retrying in ${total_delay} seconds..."
sleep $total_delay
```

This prevents all failed requests from retrying simultaneously.

</div>

</details>

---

<details>
<summary><strong>12. Reading User Input</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Scripts often need to prompt users for information. The `read` command handles this.

---

### Basic Input Reading

Prompt for input and store it in a variable:

```bash
read -p "Enter environment: " environment
echo "You selected: $environment"

read -p "Enter version: " version
echo "Deploying: $version"
```

The `-p` flag provides a prompt. The input is stored in the specified variable.

---

### Reading Passwords

Hide input for sensitive data:

```bash
read -sp "Enter database password: " db_password
echo  # Print newline after hidden input
echo "Password set"
```

The `-s` flag silences echo. Characters aren't displayed as the user types.

At ChillSpot, we handle credentials securely:

```bash
#!/bin/bash

read -p "Enter deployment environment: " environment
read -p "Enter your username: " username
read -sp "Enter your password: " password
echo

# Use credentials...
# Then clear them
unset password
```

---

### Setting Timeouts

Prevent scripts from hanging if users don't respond:

```bash
if read -t 30 -p "Continue deployment? (y/n): " response; then
    if [ "$response" = "y" ]; then
        echo "Proceeding..."
    else
        echo "Aborted"
        exit 1
    fi
else
    echo "No response received, aborting"
    exit 1
fi
```

The `-t 30` flag times out after 30 seconds.

At ChillSpot, our production deployment scripts require confirmation:

```bash
echo "WARNING: Deploying to PRODUCTION"
echo "Environment: $environment"
echo "Version: $version"
echo

if read -t 10 -p "Type 'yes' to continue: " confirmation; then
    if [ "$confirmation" != "yes" ]; then
        echo "Deployment cancelled"
        exit 1
    fi
else
    echo "No confirmation received, cancelling"
    exit 1
fi
```

---

### Reading Lines from Files

Process files line by line:

```bash
while IFS= read -r line; do
    echo "Processing: $line"
done < /path/to/file.txt
```

The `IFS=` prevents trimming whitespace. The `-r` prevents backslash interpretation.

At ChillSpot, we process server lists:

```bash
# servers.txt contains one hostname per line
while IFS= read -r server; do
    echo "Deploying to $server..."
    ssh "$server" "sudo systemctl restart chillspot-api"
done < servers.txt
```

---

### Reading Multiple Values

Read multiple values from one input line:

```bash
read -p "Enter firstname lastname: " first last
echo "Hello $first $last"

# With default split on whitespace
read -p "Enter host port: " host port
echo "Connecting to $host:$port"
```

---

### Default Values

Provide defaults if user presses Enter without input:

```bash
read -p "Enter environment [development]: " environment
environment=${environment:-development}

echo "Using environment: $environment"
```

This combines `read` with parameter expansion.

At ChillSpot:

```bash
read -p "Enter worker count [4]: " worker_count
worker_count=${worker_count:-4}

read -p "Enter log level [info]: " log_level
log_level=${log_level:-info}

echo "Starting $worker_count workers with log level $log_level"
```

</div>

</details>

---

<details>
<summary><strong>13. Exit Status: How Success Works</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Every command returns an exit status — a number indicating whether it succeeded or failed. Understanding exit codes is fundamental to writing robust scripts.

---

### Exit Code Convention

By Unix convention:
- **0** means success
- **Non-zero** means failure

This feels backward at first. Zero is true? Non-zero is false? It violates every programming language you've learned where 0 is false and non-zero is true. But the Unix convention has a elegant logic: there's exactly one way for a command to succeed and potentially hundreds of ways for it to fail. The specific non-zero value can encode what went wrong.

A command that succeeds returns 0. A command that fails might return 1 for "general error," 2 for "misuse of shell command," 126 for "command can't execute," 127 for "command not found," 130 for "terminated by Ctrl+C," and so on. The non-zero space is large enough to encode useful diagnostic information.

Check the last command's exit status:

```bash
ls /tmp
echo $?  # 0 - success

ls /nonexistent
echo $?  # 2 - ls-specific error for "no such file"
```

The `$?` variable holds the exit status of the most recently executed command, but it's ephemeral — it gets replaced by the next command's exit status immediately. If you need to check it, save it first:

```bash
./complex_operation.sh
exit_code=$?

if [ $exit_code -eq 0 ]; then
    echo "Success!"
else
    echo "Failed with code $exit_code"
fi
```

---

### Using Exit Status in Conditions

The `if` statement tests exit status directly:

```bash
if ls /tmp > /dev/null; then
    echo "Directory exists"
fi

if curl -sf http://localhost:8080/health > /dev/null; then
    echo "API is healthy"
else
    echo "API is down"
fi
```

No need to check `$?` explicitly. The `if` evaluates the command's exit status.

---

### Setting Exit Status in Scripts

Your script should return appropriate exit codes:

```bash
#!/bin/bash

if [ $# -lt 1 ]; then
    echo "Usage: $0 <environment>"
    exit 1  # Indicate failure
fi

environment=$1

if [ "$environment" != "staging" ] && [ "$environment" != "production" ]; then
    echo "Invalid environment: $environment"
    exit 1  # Indicate failure
fi

# Deployment logic here...

echo "Deployment successful"
exit 0  # Indicate success
```

Scripts that exit 0 signal success to the caller. Scripts that exit non-zero signal failure.

At ChillSpot, our CI/CD pipeline checks exit codes:

```bash
# Jenkins pipeline
if ./deploy.sh production v2.3.1; then
    echo "Deployment successful"
    ./notify_success.sh
else
    echo "Deployment failed"
    ./notify_failure.sh
    exit 1
fi
```

---

### Meaningful Exit Codes

Use different exit codes to indicate different failure modes:

```bash
#!/bin/bash

# Exit codes
readonly EXIT_SUCCESS=0
readonly EXIT_INVALID_ARGS=1
readonly EXIT_PREREQ_FAILED=2
readonly EXIT_DEPLOY_FAILED=3
readonly EXIT_HEALTH_CHECK_FAILED=4

# Validate arguments
if [ $# -lt 2 ]; then
    echo "Usage: $0 <environment> <version>"
    exit $EXIT_INVALID_ARGS
fi

# Check prerequisites
if ! command -v curl &> /dev/null; then
    echo "Error: curl is required"
    exit $EXIT_PREREQ_FAILED
fi

# Deployment logic...

# Health check
if ! curl -sf http://localhost:8080/health > /dev/null; then
    echo "Health check failed"
    exit $EXIT_HEALTH_CHECK_FAILED
fi

exit $EXIT_SUCCESS
```

Callers can check the exit code to understand what went wrong.

---

### Exit on Error

Make scripts fail fast with `set -e`:

```bash
#!/bin/bash
set -e  # Exit if any command fails

mkdir /tmp/deploy
cd /tmp/deploy
wget https://releases.chillspot.com/api-v2.3.1.tar.gz
tar xzf api-v2.3.1.tar.gz

# If any of the above fail, script exits immediately
```

This is safer than continuing after errors. Without `set -e`, a script might continue even after critical failures, causing cascading problems.

We'll cover error handling comprehensively in File 08, but start thinking about it now.

</div>

</details>

---

<details>
<summary><strong>14. Real-World Script Patterns</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Let's see how these concepts come together in real scripts you'd actually write at ChillSpot.

---

### Health Check Script

```bash
#!/bin/bash
#
# check_services.sh - Verify critical services are healthy
#

set -euo pipefail

readonly REDIS_HOST="cache.chillspot.internal"
readonly DB_HOST="db.chillspot.internal"
readonly API_URL="http://localhost:8080"

check_redis() {
    if redis-cli -h "$REDIS_HOST" ping > /dev/null 2>&1; then
        echo "✓ Redis is healthy"
        return 0
    else
        echo "✗ Redis is down"
        return 1
    fi
}

check_database() {
    if pg_isready -h "$DB_HOST" -q; then
        echo "✓ PostgreSQL is healthy"
        return 0
    else
        echo "✗ PostgreSQL is down"
        return 1
    fi
}

check_api() {
    if curl -sf "${API_URL}/health" > /dev/null; then
        echo "✓ API is healthy"
        return 0
    else
        echo "✗ API is down"
        return 1
    fi
}

main() {
    echo "Checking ChillSpot services..."
    echo
    
    local all_healthy=true
    
    check_redis || all_healthy=false
    check_database || all_healthy=false
    check_api || all_healthy=false
    
    echo
    
    if $all_healthy; then
        echo "All services are healthy"
        exit 0
    else
        echo "Some services are unhealthy"
        exit 1
    fi
}

main "$@"
```

This script checks multiple services and reports their status. It returns a meaningful exit code that monitoring systems can use.

---

### Backup Script

```bash
#!/bin/bash
#
# backup_database.sh - Backup ChillSpot production database
#

set -euo pipefail

readonly BACKUP_DIR="/var/backups/chillspot"
readonly DB_NAME="chillspot_production"
readonly RETENTION_DAYS=7
readonly S3_BUCKET="chillspot-backups"

backup_database() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="${BACKUP_DIR}/chillspot_${timestamp}.sql.gz"
    
    echo "Creating backup: $backup_file"
    
    pg_dump "$DB_NAME" | gzip > "$backup_file"
    
    echo "Backup created: $(du -h "$backup_file" | cut -f1)"
    echo "$backup_file"
}

upload_to_s3() {
    local backup_file=$1
    local s3_key="backups/$(basename "$backup_file")"
    
    echo "Uploading to S3: s3://${S3_BUCKET}/${s3_key}"
    
    aws s3 cp "$backup_file" "s3://${S3_BUCKET}/${s3_key}"
    
    echo "Upload complete"
}

cleanup_old_backups() {
    echo "Removing backups older than $RETENTION_DAYS days"
    
    find "$BACKUP_DIR" -name "chillspot_*.sql.gz" -mtime +$RETENTION_DAYS -delete
    
    echo "Cleanup complete"
}

main() {
    mkdir -p "$BACKUP_DIR"
    
    local backup_file=$(backup_database)
    upload_to_s3 "$backup_file"
    cleanup_old_backups
    
    echo "Backup completed successfully"
}

main "$@"
```

This script performs a complete backup workflow: create backup, upload to S3, clean up old files.

---

### Deployment Wrapper

```bash
#!/bin/bash
#
# deploy.sh - Deploy ChillSpot services
#

set -euo pipefail

readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly LOG_DIR="/var/log/chillspot"

usage() {
    cat << EOF
Usage: $0 [OPTIONS] <environment> <version>

OPTIONS:
    -v, --verbose       Verbose output
    -h, --help          Show this help message

ARGUMENTS:
    environment         Target environment (staging|production)
    version             Version to deploy (e.g., v2.3.1)

EXAMPLES:
    $0 staging v2.3.1
    $0 --verbose production v2.3.1
EOF
}

validate_arguments() {
    if [ -z "$environment" ] || [ -z "$version" ]; then
        echo "Error: Missing required arguments"
        usage
        exit 1
    fi
    
    if [ "$environment" != "staging" ] && [ "$environment" != "production" ]; then
        echo "Error: Invalid environment '$environment'"
        exit 1
    fi
    
    if [[ ! "$version" =~ ^v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
        echo "Error: Invalid version format '$version'"
        echo "Expected format: v2.3.1"
        exit 1
    fi
}

deploy() {
    echo "=========================================="
    echo "ChillSpot Deployment"
    echo "=========================================="
    echo "Environment: $environment"
    echo "Version: $version"
    echo "Started: $(date)"
    echo "=========================================="
    echo
    
    # Create backup
    echo "Creating pre-deployment backup..."
    ./backup_database.sh
    
    # Deploy code
    echo "Deploying application..."
    ./deploy_app.sh "$environment" "$version"
    
    # Run migrations
    echo "Running database migrations..."
    ./run_migrations.sh
    
    # Health check
    echo "Verifying deployment..."
    ./check_services.sh
    
    echo
    echo "=========================================="
    echo "Deployment completed successfully"
    echo "Finished: $(date)"
    echo "=========================================="
}

# Parse arguments
verbose=false
environment=""
version=""

while [ $# -gt 0 ]; do
    case $1 in
        -v|--verbose)
            verbose=true
            shift
            ;;
        -h|--help)
            usage
            exit 0
            ;;
        -*)
            echo "Unknown option: $1"
            usage
            exit 1
            ;;
        *)
            if [ -z "$environment" ]; then
                environment=$1
            elif [ -z "$version" ]; then
                version=$1
            else
                echo "Too many arguments"
                usage
                exit 1
            fi
            shift
            ;;
    esac
done

validate_arguments
deploy
```

This deployment script shows real-world structure: argument parsing, validation, orchestration of multiple sub-scripts, clear output.

---

These patterns appear repeatedly in production scripts. Argument parsing. Validation. Logging. Calling other scripts. Error handling. Clear output. Now you understand the fundamentals that make these patterns work.

</div>

</details>

---

## What's Next

You now understand Bash fundamentals: what it is and why production systems depend on it, how the shell environment works, how to create scripts that execute reliably, variables and their scope, quoting rules that prevent bugs, special parameters that give you control, environment variables that configure behavior, parameter expansion for string manipulation, command substitution for capturing output, arithmetic for calculations, reading input safely, and exit codes that communicate success or failure.

These are the building blocks. But scripts that only store and manipulate data aren't very useful yet. Real scripts need to make decisions. Check if files exist before trying to read them. Verify that commands succeeded before continuing to the next step. Test whether values match expectations before proceeding with operations. Compare strings, test numbers, validate conditions.

That's where conditionals come in. Every robust script is full of `if` statements checking preconditions, validating input, and handling different scenarios. Understanding how Bash evaluates conditions is what separates scripts that work carefully from scripts that work reliably.

**Next up: File 02 — Conditionals & Logic**. We'll explore how Bash evaluates conditions using test commands and exit codes, the different test syntaxes and when to use each, file tests that check filesystem state, string comparisons that validate input, numeric comparisons for thresholds, logical operators for combining conditions, and the patterns you'll use daily to make scripts respond intelligently to their environment.

---

*You've learned how Bash thinks and stores data. Now learn how it decides what to do with that data.*